```
hahaha [Applause] okay so good afternoon for me it's a really pleasure to be here with you and I'm gonna share about how we are integrating our own cloud and on-premise reports since we are a non-cloud native company uh I'm Claudia sephrine Claudia sifrin in Portuguese I'm Brazilian and I'm the Phoenix manager at Grupo bochicario our agenda today so I'm gonna start talking about Grupo bochicaro uh how we are doing synops at group of Chicago our journey our technology Journey how do you operate in on-premise and on cloud models nowadays and some lessons and learnings during our process uh so who we are group is the largest network of Cosmetics franchises in the world we are a Brazilian company with 46 years old now we are present in all across Brazil and in another 50 countries including here us you can find our products in obojicario.com if you like we are the beauty Commerce leader in Brazil we have factories we have distribution centers of 15 000 employees sustainability is on is in our role and something that we are really proud of uh and uh here is something that it's really interesting uh we are a complete ecosystem of beauty because we go from the industry to the final Point of Sales so we have all the chain we have the logistics uh we have the Laboratories we have all technology stuff so we are kind of complex so we have lots of information going on and uh for our consumers for example we have different digital products for our non-proprietary channels which are like markets pharmacies we have and other ways of working for our franchisees for our resellers for our e-commerce for our marketplaces so we have lots of channels and we have lots of consumers in all across the world which became a kind of fun to work with and we have lots of Brands we are well known by our main brand that is obochikaru but you also have like a and somewhere okay so how about finops at Group Chicago uh nowadays we are one of the most mature companies infinops in Brazil we have Cloud indicators linked to our business so we measure indicators for example cost per order cost per users we are studying cost per Revenue right now so we are able to link our Cloud costs with our revenues and Link that to our finance guide to our strategy most of our consumptions are inefficient purchasing models right now we are moved Cloud so we are running in the the three main providers and we also have data centers so we also have private cloud and something that we are really proud of is that we have 90 percent of visibility of our taggable costs this is really a great achievement for us because while we are migrating our applications to the cloud we are of course increasing our billing and we are able to take everything in a standardized way so 90 of visibility for us Euros are really really really work that it has been done and something that we are really proud of um okay and talking about my team so uh first things that I that I want to share with you is about how we we talk about synopsis we don't just do Cloud uh cost controls so our scope infinite infinopset group watch Caro we are controlling all the operations technology so all the financial involving uh Cloud control involving on-premise controls involving licensing involving contracts strategy so capex everything is under our scope so we have lots of things to do we have a huge scope and for that I have a multi-disciplinary team so I have people with financial skills I have people with technical skills as well so we can mix everything together and we can have different approaches um but okay so I've shown you some indicators I've talked about you about grupobo Chicago and how did you get here so how we started all of this stuff um so in 2019 we started a really great revolution of Technology Revolution at Grupo Chicago in these four main points here that I'm gonna show you uh our infrastructure uh the ways we architect things the way we deploy things the way we integrate our platforms so this was kind of a big like a big bang really and it changed completely the way we work with technology so in order to achieve everything that we were going further we have to change lots of things including people so in 2019 we were 300 people working in technology and nowadays we are almost three thousand people working in technology that's why we decided that's because we decided to build everything in home so all of our application most of our applications nowadays they are built at home and so for that we have to to modify everything and to be to have this scale to gain flexibility to being able to do that of course we needed to change from just on-premise model and to move to the cloud and so talking about infrastructure that's what we did so on 2020 we just have like data centers we are a majority on-premise and environmental and when we started thinking about moving to the cloud we started thinking about finops as well so one thing that we really achieved as a team there is that we integrated phenops since the beginning since the beginning of our migration process and this was a completely different from another Enterprises that we know and it's for us it makes a really different approach so um in 2020 to 2021 we made a huge study we made a huge business case to understand how we are going to deal with our costs so when the on-premise cost is going to decrease the cloud cost is going to increase so how we're going to track that we made a business case for the the next five years and we are tracking that every month and so in 2021 we approved all the migration and we started that and then we started different Ops controls we already had the on-premise controls the licensing controls so we already had a team a team who are taking care of that and then we integrated this team with the cloud team and then we started building our reports so in 2022 we say that we formally Lounge synopsis so we have a huge team with all of these Scopes and all of reports and something established and nowadays in 2023 we are running and evolving and searching for Best Practices that's why I'm here so learning a lot from you uh okay so this is a framework from a working group that is uh currently uh working here in a foundation the and they are studying the intersectionalities between uh finops and TBM the technology business management and when I look at this framework it that is exactly what we are doing in real life at grupobochikaru because we are integrated all the information and all the reports and the way we do with Finance operation um so I chose here four topics to share with you about how we are doing that so I'm gonna talk about showback chargeback budget management and optimization and last I'm gonna talk some lessons and learnings that we have during our process so to start here data analysis and showback so uh reading in grade years you're finding something that are common in our process for both models for on-premise model and for cloud model in green letters so you'll find something that is specific for cloud and in Orange letters something that are we have is specific for on-premise model so data Automation and normalization so this is a common we have to do that in on-premise models we have to do that on cloud models so how can we normalize how can we show this visibility to our stakeholders that we need so we have to speak the same language besides the model we are operating besides if it's cloud or on-premise we also have presentations that we made for managers for executive teams for technical teams and we do this like in a monthly release so we have different approaches for both teams for both third three four teams and we do that with information from cloud and with information for from on-premise Models so we integrated our reports and we present this depending on the the teams we have dashboards we've discussed you for people they can use they can consumer data and we are communicating our phenopsy stuff into his live channels so we use slack we have a slack Channel with every team that we have across our company we call every team there for at vs like value stream so we have a slack Channel with every value stream that we work for so we are a centralized team but we work across company and then we share these reports with them in these live channels and we do this information we share this information for both models as well for cloud costs and for on-premise costs and something that we do just for cloud for example is reports on clusters so we we're just running kubernetes in Cloud we're not running it into on-premise so we have reports specific reports on cluster for example and it's just working on cloud and tagging status report as well so we don't have tagging uh or at least we don't call tagging in on-premise environment so we have a specific reports on tagging and we are just running that in Cloud models and for on-premise so we have capex so we have to uh to track our cash flow we have equipment so we have to monitor our depreciation how we're going to deal with this costs and so we have different approaches for that and so we have different dashboards and reports on these matters and some challenges that we have here and the challenges again they're common for both models that we operate so get the attention of everyone so be assertive in how we format the information and give visibility to the right personas to the right people so these are challenges that we addressed for both models so um when we're we're talking Choice active they want to know how much are the results how are the the things that are impacting our business how the cloud costs the operation costs are going to to go with our strategy and when we are talking with technical people we have to give them more operation indicators we have to give them more deep data so they can take care so they can try to understand better in order to to be more awareness with our costs so I mean something that is really challenging is how many data how many information in which format we do for the our different clients for example uh the next topic here is chargeback and finance integration so something that we really do for both processes is the monthly alignments that we have with our finance team with fpna so this is something uh nice that we created since the beginning of we migrated we already have alignments when just talking about on-premise model and when we are operating in Cloud right now we have these both alignments with our finance team and it's really nice it's working really well we have created a center of course to a specific allocate our expenses in cloud and our expenses in on-premise so the finance team are able to track everything that we are running and we can like talk with them in a more easily way because they know how much we are spending because they can track that in in coastal locations correctly and we we can talk with them because we already know how much are the numbers because we we made we created that numbers for them and something that we do and again this is something for us I'm not sure if it's uh like um something that everyone should do but in group Chicago we don't do copics for cloud expenses I know there are some companies that are looking for into right now but we don't do that but another way that we kind of separate we categorize more these Cloud costs into in accounting models we made some clusters with our finance integration teams so we are running for example costs called costs into portfolio models so something that our digital products that are Innovation that are something really new or something that can like really grow we track that as portfolio costs and something for example it's daily something we are running for a long time we track that as GNA costs so we have we categorized some costs including cloud and including on-premise in in order to have more accountability with our financial team and so we do amortized our licenses um besides we acquire them into marketplaces in cloud or we acquire that in the traditional model we are using the same process of amortization and so something that is really different right now from both models is how we apportion this costs for teams so talking about cloud we are able to apportion this to every team because of the tags and we are not doing that from on-premise yet we are running some studies and some books right now to for us to be able to track more and like to decentralize these costs but nowadays we just apportion costs to the areas uh on talking about Cloud so talking about on-premise we have decentralized yet and some challenges here so meeting some fpna deadlines sometimes because I'm not sure if you if you if you're familiar with that if you're faced with that but sometimes they they came with okay we need a study we need a number and it's going to be for tomorrow and oh okay tomorrow however am I gonna do that so this is something it's challenging and in both models as well and automation so I'm gonna talk about Automation in this topic and in the next two topics as well because we are always trying to evolve in automation talking about chargeback and other stuff so the third thing here is budget management so our calculation are ways of do budgets for both cloud and on-premise model they are kind of the same and we use something that we are just running for on-premise to now use that on cloud as well so we do that on based on history costs and we also have the seasonality so we we have models to to build budget for the next 12 months for the next 36 months and this is something really nice and we are like integrating this costs so the our models for both operations so for both cloud and on-premise we have inputs from technical teams about new things about growth about new demands that we are not seeing yet so we put everything we asked everyone to came across with us when we are talking about budget because we need their inputs and we do that for both models so for license for licenses for our suppliers on premise we have to to get the inputs from the teams we have to know okay this contract we are going to increase or you're going to decrease so this kind of stuff this kind of alignment we are making we have to make sure that every number is okay and we do that for cloud model and for on-premise model and we have to make budget in two currencies so here in the US you just make budget into US dollars so for us we have to make current uh our budget into US dollar first and then to convert that to AIS which is our currency because uh just in on-premise we have a farming suppliers and of course in Cloud we our suppliers they are from here so we need to do that and we do that for both models as well and the alignment with fpna again so during all the whole process of budget we have to make sure that fpna people they are aligned with our numbers and so here we decentralized the budget for cloud and we don't decentralize that yet for on-premise so for on-premise we do that in a centralized way we take care of that and for uh for cloud costs we decentralized that which means every value stream every team has his own budget to take care and to help track the costs and some challenges talking about budget automation again so how can we be more agile how can we be more faster talking about budget to identify allocation changes between teams so sometimes we have a squad and they're running into some vs and because of a decision a strategic they change so we need to make sure that the budget allocation is going to be correct for this new team for this new allocation so this is a challenge sometimes shared costs I don't know if you are facing with them but how can we be assertive when talking about shared costs so nowadays we do a proportional methodology we are sharing a hundred percent of our Cloud costs so we decentralized everything but I mean every day we are trying to evolve on this shared cost to be more more assertive and to collaborate so how can we guarantee that 100 percent of people that needs to be involved are really involved so this is a challenge for both models we need the collaboration talking about on-premise we need collaboration talking about cloud and so this is a challenge that we Face together and we try to resolve together as well and uh the last topic before the lessons and learnings here so optimization and what I mean by optimization I put everything into this box like anomalies research resource utilization managing management management of commitment so for me this is all kinds of uh optimization process so we have cost reduction Targets on both models so we do targets for cloud costs we do targets for on-premise costs and we share that with our teams so they can work on that and we track that so the second topic here it's the reports so we give visibility to how much they need to to achieve and how much are they already achieved so how many savings are the teams getting how many say how much saving they need to work on and for that we also also have agendas with our teams to discuss these saving opportunities so we understand that it's a phenops job to try to provoke our teams into these opportunities as well so you don't just give dashboards to them but we also have meetings with them and we try okay look I'm knocking into your door and I'm saying oh this is the opportunity these are the opportunities so you need to take a look at that here are different ops team and I'm going to share this data with you and I'm going to help with you you with that so we made agendas for both for cloud optimization for on-premise optimization as well so and this is a process that we do together so here in this topic is where we have the most difference between our models so talking about Cloud well efficient purchasing models we just have this on cloud so savings plans reservation spot code we do all of that but just on cloud and we have reports in separated for just Cloud cost alerts so we use the cost alerts from our providers and again this is just configured for cloud costs we monitor their our underutilized resources and we have recommendations to optimization and we are doing that just for cloud today we are evolving this to do for on-premise environmental as well and that's why we're running some studies some box trying to understand how we're going to do that but nowadays we're just doing that for cloud and I mean in on-premise we also have things that we can optimize so for example the use of licenses so we need to know how many people are using that slices uh how are the unit costs of some license if their user is really active or not if not can we cut this license so there are lots of ways to optimize costs into on-premise word as well Telecom resources are another thing so how can we make sure that we are being efficient in talking about mobile talking about internet talking about links so this is something that we are really looking into talking about on-premise and some challenges that we have here in this process so the engagement team again so like future things but we need everyone to engage with us in order to achieve our results in order to to achieve our targets or optimization efficiency data so how can we provide more visibility more data to teams so they connect and automation again automation so how can we be more agile more faster we are working on that right now so automation is something that we are really looking into and uh something that it's really um important for us and it's a challenge right now for cloud costs is improve efficiency into containers so we are looking and how you are going to improve our idle resources so this is kind again this is kind of there it's just for called Environmental uh okay so to finish some some learnings that we had during our process uh since now so the this collaboration that we have from teams so people working with Cloud people work with on-premise and how can we integrate them to discuss best practice and how we're gonna resolve our problems are going to solve our problems this is really nice and we can we have great discussions about that so business case mostly follow-up so being able to track our business case since the beginning until today this uh is really a nice job that we are doing and of course our company is happy because we are showing them our numbers we are showing that the business case that we made before moving to the cloud is actually being tracked to have homogeneity of information so running on premise running Cloud you have to to normalize your data you have to to give visibility in a way that people are going to understand and the last one here engagement with technical teams so again I'm sure you agree with me that phenobs we don't do phenobs by ourselves we don't do phenops alone we need to engage with everyone and so this is really a an important lesson that we have since the beginning and that's it thanks for your attention and now I have I think we have some time for questions yeah we do have some time for questions you were first all right thanks for that um I assume you have some teams that are that are running workloads in both the cloud and on-prem when you're doing uh reviews with those teams and when you're showing them dashboards and you know showbacks and all that do you treat it as you know you're an on-prem team this is your on-prem stuff this is your Cloud team stuff or is there more of a holistic like this is your total budget and your total usage so we have uh separated dashboards uh now but we are working on integrated then to give the divisibility of the total operation costs but we don't say oh you are running uh private so the here is your numbers no there are teams that they are actually running in like two clouds and one more like in private and we showed the data and we're not like saying to them oh you are private so here's our numbers no we don't separate this way but our data now our dashboards they are separated and we are working to integrate them you're doing a lot of different functions and you mentioned the big team how big is your team managing budgets and spend in reports and things like that okay uh today we are 16 people last one before you started your migration to the cloud how granularly did you measure your infrastructure costs and usage had you always done that or was that something you started doing with the cloud optimization uh no before migrating to the cloud we already have some dashboards some views on our on-premise environmental but I mean they were they weren't really deep so when we migrated to the cloud and when we know okay so there's tags we can go deep into Cloud costs let me go now to on-premise environmental and try to do the same thing so we kind of uh get the best of both worlds and integrated so now our data from on-premise they are better than they are before that's it thank you so much Claudia for your presentation was really beautiful you thank you thanks for watching that session I'm sitting here in San Diego right after phenob sex we hope you join us next year here live 2024. in the meantime please subscribe to the channel and join the community get involved join the Summits get in a working group and don't forget to get finops certified because next year here in San Diego for finopsex it's going to be twice as big come join the party come meet your people welcome home
```


## Transcript 2

```
all right welcome back everybody I hope you had a nice cup of coffee maybe some chips and salsa and you're ready to go let's give a nice warm welcome here for denesh sonani and Don rudish from [Applause] workday we got a very timely topic for you here why we built our own cost governance tool this is great I think it was just today the tools and services working group published uh a paper on building versus buying so we're right on a timely topic here thank you guys so much let's kick it off yeah thank you thank you thank you well dases well let me take a step back and since I have the audience here I'm going to ask we built out this new cost governance tool for workday help save millions of dollars am I and my team going to get a bonus after this Don in front of all of these people seriously that's the point if you deliver on the road map that you promised I'll take you and your team to a fancy restaurant how about that fancy restaurant that sounds good let's see how fancy you can get so let's uh we built out the tool but what were you using before we built out this new platform yeah so don our needs earlier on were very simple so we used to use the csp's native building consoles and as our Engineers started to use cloud we became multicloud we had to do something so we implemented a thirdparty package software and it served its purpose up until a particular point in time and as our Engineers became really cloud first we had to do something different so we had to take the difficult decision the hard decision to build our own tool and we still enhancing it it's a journey that's continuing to grow as we make progress so this is where we are now and you know why we had to build our tool so you know the cost is kind of in a third package software or a billing console you get the cost per instance or cost per this EBS volume but we wanted to bring in the business context to that data right so you know when you say unit metric is always denominator which is the Enterprise data so when you say cost per customer the customer and the Tenant data is in our own data warehouses so how do you blend that do you bring that data into the third party package software or if you run that depth test how much did that particular test cost us that test information isn't with us in Enterprise how do I blend that with the third party package software which has just got the numerator not only that for us public cloud is material it impacts our EPs and our finance colleagues used to always ask us what is the variance why do you have such a high variance between your forecast and your actuals and you know how people used to analyze that how the best software tool out there spreadsheets not one not two a plethora of spreadsheets so we had to do something different so we build a planning module inside of our cost governance tool and the last quarter that we closed as you know Dawn our forecast accuracy was 99.5% just 5% variance that's impressive 5% forecast variance you know uh especially considering I remember back in a time when forecast managers would come up to us a bit puzzled saying that you know my spending actuals are dramatically shifting from month to month even though my resource usage remain pretty much static so what did we do to help out these forecast managers and they were all right I mean for the same identical usage period over period week over week the past dollar could change because the way the commit based instruments RI savings plans cards are applied you could have dramatic shifts in your cost so what we did is we implemented something very unique which is we implemented standard cost the clicker is acting on its own so we got at the start of the year the unit price for every skew that we use is fixed it doesn't change throughout the year so only thing that can change your cost C is by the type of instances you use instead of Intel instances using AMD instances or arm architecture brace processors or how long do you use and do you shut down so that is what made our life easy and all of these allocations and everything becomes very easy nice now so it it makes a lot of sense to to have these teams really focus on standard costs that way they don't get bogged down by worrying about these variables like savings plans Reserve instances and they can really just focus on the key drivers of cost which is resource usage but I guess what that means is if we're building out this new tool right we we need to have uh we need to keep these CSP discounts under wraps and just restrict that to the core finops team totally and as a result not only the what data but the way the data is presented right the leaders want high level summary Engineers want detail you don't want a clutter of 30 different filters on the top and then you don't know which filters to click not we need Persona based dashboards so all of these were limitations that we were observing in the third part package software nothing against that software it served its purpose but I had this all these inherent limitations how do I bring in my own data yeah I guess I guess the benefit of building out your own tool is their ability to customize it so we'll be able to integrate into our service registry so that we can get contact details for account owners and send out automatic alerts via slack or email when you know there certain thresholds are being uh reached we can even normalize on Cross uh CSP service definitions across multiple different providers totally and all of this is fine right I mean the what all of this gives you the answers the what questions what is my spend for an account or a gcp project or for this instance all this what is table State what is really required is what can I do about it the engineers are interested in what actions can I take to reduce cost or optimize and add value to the company and this theme as you know through the foundation survey is common across not only workday but the entire community of how do we reduce bed how do you get forecast accuracy we believe we did the tooling to help us get there by creating our own tool called optimize platform usage and spend so without further Ado we just want to show you some dashboards images we can't record an actual video so we got some gifts to show you over to you Don okay thank you so here we have the Opus landing page here we created a tile for every finop subject area where there's a set of links so that when a user clicks on one of those links they're redirected to a dashboard that already has uh the filters pre-populated for them for that context on the next slide going to demo two of the views that are available to users so here we have our usage summary tab where users can see spend across products and services and then our waste elimination tab where users can take action items items on cost savings opportunities there are two additional tabs here that we are unable to show you right now but it's our forecast accuracy where users can see the trends of their forecast accuracy over time as well as the budget flux to compare their spending actuals against budget but there are two additional features that we'll be able to demo on the next slide so on this dashboard we have a dashboard that is defaulted to the weekly view but we can change that to the day View and then also filter on let's say Saturday and Sunday in order for us to see maybe our resource usage on the weekends on a development environment for example with that I'll pass things back to denesh and we go over more features thank you Don so what you see here is uh kubernetes implementation on the x-axis you have all the different workday services that make up workday application and on the Y AIS you have the utilization of the service against the requests and the limits that they have scheduled for uh kubernetes to provide So Below the line let's say 66% is good enough utilization you can see all the services who have requested more resources be it CPU or memory this charg is showing memory but we have also CPUs and that is dollarized so that the individual service owners get a view of how much opportunity do they have to save cost you can always drill to detail this is standard feature of almost all bi tools so let's say I'm drilling down the product hierarchy now Focus project gets us maybe the crawl phase we are multicloud what let's say AWS calls as a memory optimized instance Google calls as a general purpose instance for example we want to know how many Engineers are using AMD or Arm based processes because they are typically cheap cheaper than the Intel B processor so we had to conform multicloud into this product Dimension so that the engineers Can See For example a spend for ML how much is that on Intel processor versus AMD versus a GPU irrespective of the CSP that is rented on right as of now if I have a package software I have to go to One dashboard which is AWS only or gcp only so this all kind of brings it home this is our secret Source we have a metadata which is a religion our team maintains that every Friday any change to the account owner the cost center who is paying for it who is going to do the devops and optimize is maintained religiously really religiously this is what it makes happen so a little bit about work day we are a Fortune 500 company as our founder CEO uh to quote him we help customers transform how they manage their two most important assets their people and their money we tens and thousands of Fortune 500 companies extremely high customer set 95% plus we process 3 billion plus transactions per day basically we are a software company and you saw my first slide we took a long time to build our own cost governance tool so why did we took that that much of time we had to get certain Basics foundational aspects under control and then go into tooling tooling comes last for us it is people first processes and the tools last so we learned from all using this package software and let's say tool for optimizing kubernetes discussed and we pulled a proposal for our product and Technology uh leadership team to get the funding to have the right talent and not only the phobs talent but the engineering talent to build this and then we focused on all the processes and policies and compliance and governance aspect to make that happen so let's go through each one of them a bit um at the highest level we have this accountability metrics we have a hub and spoke model at the Center is our ccoe and the phop team and we have group what Dawn showed you was our landing page was a way to declutter when you are looking for your subject area you don't want to filter if you just click that you go there likewise all the subject area owners has a finops counterpart and a devops counterpart so that they can be held accountable for the uh optimizations it's like that Spider-Man meme doesn't happen who is responsible right it is all avoided through this this is our superpower that I mentioned about almost like a religion our approach is many small accounts rather than one large account because of a security and policy approach that means we don't need to necessarily tag because this metadata helps us understand where the spend is which cost center so that our external segment reporting whether it is R&D or sales and marketing or cogs is very clear we don't have any pollution inside an account that you have R&D workload sitting there as well as sales demo in workloads or your PR workloads no that's just a no no every account or a gcp project has a platform owner not only the current active but the forecast because we are so strong on our planning uh accuracy it's our number one kpi we have when we do our long-range plan we add to that list of the future accounts and gcp projects that we're going to open up and we submit the forecast and the plans uh to that and of course uh the hierarchy we you have thousands of accounts you don't want to look at the trenches view we want the Forest View so we let's say we want to measure our spend across ML and ml might have let's say 50 different projects one is for your training one is for your deployment for this segment or whatever that's rolled up into a hierarchy so you start top down so this really made our life extremely simple we don't even need tagging if you get this under control the reporting becomes much easier let's take an example of kubernetes implementation let's say there are two accounts One account has your infrastructure security networking Hub and the other account has your two clusters out of the box kubernetes labels really well uh through Prometheus you can suck in all those labels let's say this cluster has a name you don't need to tag because you already know that cluster name is for that particular application likewise each node or you have node groups you don't need to tag because it already is available let's take the last one which is environment that are in this hypothetical example there are five parts we know the cost of that node let's say say WS are ftic large instance its usable memory is 256 but we can really deploy 249 gig of workloads we can allocate the cost down to each pod based on the request that the Pod has made to the uh to the the control plane so we do the cost allocation direct as I just mentioned for all the parts and containers all the stateful set also but we borrowed this cost allocation from manufactur ing manufacturing has been doing standard costing cost allocation ever since manufacturing has been existing right so this is not new maybe it's new to fops and public Cloud so in manufacturing World there is a concept called burden or overhead like your HQ it's not allocated to the product it's just kept centrally so we don't charge that it's all those kinds of services which are Central those are kept centrally and they're optimized centrally but there is something in between right so the analogy that I was talking to you about right let's say office space cost allocation so you can allocate the cost of the space and say okay whatever number of seats who's occupying I can do that but you also have toilets in the office and you have kitchen you can track okay you use a toilet for 5 minutes and allocate the cost but is it really going to move the needle so you need to be really careful and we were extremely d read what cost to allocate and what not to allocate standard cost I mentioned about this so what we do is we hold the unit price of that all the skes that we use constant at the start of the year so that horizontal line let's say is the CU price for the entire 365 days and the dotted line are the variability that will happen through the year now finops team is held accountable to get the actual cost and the bonus you talked about get the cost actually lower than that and then you can earn the bonus right so yeah that really helps then your P * Q based on your usage if your usage is the same you'll just have flat cost dollar that's really helped us really uh achieve this we got the next aspect which is our rhythm of business as we call it or your governance aspect we do long range plan twice a year because cloud is so fast changing new projects optimization in fact we just concluded as you know our long range plan but we do meet with all the leaders every month that's where we look at the forecast the variance what happened root cause analysis blameless reviews that has really helped us get the accuracy to the high level that I mentioned and uh the way we implemented our forecast module is really the key so at the heart of it is this concept called your plan series let's assume we are in the month of June we are in plan series V6 so we have actuals up to May month and we going to do forecast rolling 12 months June onwards let's say we had call the forecast for August $123,000 fast forward 3 months we are now in v9 we have actuals up to clicker if it works we are in September month right v9 9th month we have actuals up to August and we can see the actual in that v9 was 138 and now versus the forecast of 123 so we have goals for each leader 3mon forecast accuracy of plus or minus 3% and 6mon accuracy plus or minus 5% and this tooling has really helped us so what we do is through Opus which has all the detail transaction level detail we pull all of that at a day level aggregate put it into our planning module and that triggers for all our hundred hundreds of forecast owners to submit their forecast and each one of them have detailed forecast models which are driver based based on which skew are you going to use how many customers are you going to on board in one particular region how are you going to optimize all of that is fed back into the tool and then we create all this and retain all this puras all this 12 plan series two long range plan series and series and one budget series and we also have the bigquery ml model which predicts what the forecast is going to be or what it thinks it'll likely end that month with that's compared with the forecast that is submitted and it gives an anom anomaly alert to the team why are op stash for so this is all made that all the governance life much easier so with that I'll hand it over to Dawn to talk us through the architecture of Opus so dases do you know what uh it reminded me of when you reached out to me way back when to say you know we wanted to build out a new platform for workday for finops you're talking about hair or something naughty I guess nothing naught no no no nothing naughty I'm still trying to get that bonus out of you after all but uh it reminded me of this commercial back back in the '90s where SC Sperling would come on at the end of the commercial and hold up a little picture of himself and say I'm not just the president but I'm also a client for her Club for Men that's because we're now entering this territory of practicing what we preach and this is for you folks in the audience you know uh we might be the creators of this new cost governance framework for finops but that means we're also a service owner so every month Danesh has me sit down with a member of the core finops team and I go through a cost breakdown analysis and I talk about the customized allocation uh methodology that we use to break down the Opus cost to the individual users who are using this internal platform go through budget and forecasting uh forecasting out the 12 month estimates and then I walk away from those meetings with action items for cost savings opportunities and action items so even though I'm part of this finops family it's like working for your debt right but what this does do is it gives you a unique vantage point because while we're trying to on board other applications team onto the finops platform we understand some of the pain points that they're going through because well we've done it and so we're leading by example in many uh in many cases but it's still a challenge to onboard applications team onto a finops platform because let's face it you're still asking them to do some work so a technique that we've uh that we're doing is we are leveraging technologies that allow these application teams to continue to use their existing Data Solutions now these Data Solutions can be observability solutions that they're using for performance monitoring uh measuring resource utilization or measuring transaction volumes or even going through uh user auditing or environment configuration details the point is or the key here is to stand on metadata that's because if we standardize on metadata we can uh standardize on things like resource identifiers both from the operational data sets as well as the financial data sets that are coming in from the cloud service providers and if we do this then we can have tailored analytics for each of these subject areas to give you a breakdown on maybe cost per customer or cost per transaction or if you're an internal service organization like Opus cost per user now how do we do this we're uh let me talk about three main open source technologies that are powering Opus the first of which is trino so trino is our unified query engine and it enables us to connect to various uh to multiple databases that are spread across multiple accounts and platforms now this is despite once we query off of these we can query off them using one single query language now the databases that you see attached here are not physically part of the finops data Lake per se instead they all are operating under you know the the owners of those databases and so the data remains in place we don't have to duplicate any of the data in order to integrate these data sets together moving over to the middle section of the slide is DBT so DBT is the industry standard for doing data Transformations but for us it's a bit more than that because it enables us to have multiple teams developing onto our common data platform simultaneously as it has a good platform for doing workflow governance and uh a mechanism for doing rooll and access controls on individual models so that's a perfect platform for us to implement our data governance solution the last open- Source technology is light Dash so light Dash is an open source equivalent of Google looker and it integrates seamlessly into DB allowing us to have that that governance in place but then also allowing for self-service analytics it has advanced visualization capabilities and collaboration capabilities that's it integrated into slack and email so that's what we're using to send out alerts to various account owners when they're exceeding certain thresholds and one thing that you probably some of you might have noticed on the dashboard demos that we gave there were all bar charts or some lines there were no pies or Donuts so if you do a Google search leave the pies for dessert we will see why there are no pie charts in any of our dashboard visualization dases doesn't like pie charts I learned that early on on the next slide we'll go into the data composition that's uh that's encompassing Opus first we have our our core finops data that comprises our CSP build our forecasting our CSP discounts now this for work dat ranges into the hundreds of gigabytes per month but it's a it has a very broad impact as it's integrated into almost every other data set uh within Opus now this is in Star contrast to our operations data here they have a relatively localized impact uh for for each subject area however the data volume for our operations data is massive we're talking about hundreds of terabytes per per month and pedabytes in total now this was a key consideration that we had to take into account when we were deciding on where we were going to deploy the Opus platform because we wanted to make sure that we are avoiding massive data transfer fees when we're integrating these data sets together so we stored this close to the data gravity and we're using Regional Services where possible to avoid those fees yeah that's a very important point because we learned through we had one implementation we had to scratch because we wanted to be collocated by the data is that unit metrics is a big lofty word it basically is a numerator and a denominator right so you to get the denominator which is your Enterprise data that's where you need the tiny data set of cost which is your numerator to reide otherwise you're pumping petabytes worth of data into the other side and just incurring too much so we eat our own dog food so he's got a kpi to have the cost as low as possible and you saw all the open source technology which makes us be as lean and practice what we preach yeah that's right that's right now there are three other data sources that comprise Opus the standard metadata that we covered in the previous slide but then there's also third-party apis because not everything can be integrated or not everything is a database that can be integrated into trino and then last we have our documentation that helps Point users into the right content now one of the goals of Opus is to democratize data finops data to to all of our workmates but we want to be very intentional on how we give out this access it's kind of like the uh prime directive from Star Trek any Star Trek fans out there okay for those of you who are not a Star Trek fan you're missing out but also here's the P here's what the prime directive's goal is you know we want to protect unprepared civilizations from the dangers of introducing them to new advanced technology and knowledge before they're ready for it that's why we introduce most of our new finops users to standard cost but with that said let's go back to our finops uh platform access at the base of the pyramid we have our data integrators these are our Gatekeepers into trino they're responsible for bringing in the new data sets but then also Cur curating that data sets to be accessed by the business intelligence layer advancing up the pyramid we have those dashboard Publishers these are subject matter experts that are taking those curated data sets and they're responsible for producing quality charts and dashboards that are going to be used by the masses and finally we have the consumers the consumers are using data uh using charts and dashboards that are produced by others but what unique about Opus is that these consumers still have the ability to do Advanced analytics they can do filters and drill downs and create their own charts however they cannot publish any of the charts that they um that they produce they have to work with one of the subject matter experts who has dashboard publishing access and this is our way of maintaining good data hygiene within the environment while still allowing for self-service yeah and it also helps avoid the database or dashboard sprawl because that's what one lesson we learned that if you allow everybody to create the dashboard it's just a slight tweak to the existing dashboard and then your overall user stats are lower and people are just making their own Fork dashboard with a different data definition that's right and then it becomes very hard to figure out what the authoritative source of Truth is and which dashboard should I be looking at so as we wrap up this is the full text stack of Opus so we're using open source and then we're also using commercial software on the left hand side you see the open- source core Technologies and we're using this primarily to one obviously reduce licensing cost but then two the customization functionality that this provides but then we're also still using commercial software what makes sense to do so so for example we're using slack all throughout workday so it made sense to use that for our Communications and alerting for our anomalies we're using Google Sheets for uh not for big data analytics but we're using it for data input for forecasting and then big query ml as it has a lot of ml capabilities out of the box and we're not sending massive data sets over to Big query so the cost is relatively small now Apache airflow is open source however we're using the cloud service provider managed uh instance of Apache airflow because our use case for it is simple and it's uh inexpensive so that's a good segue into our lessons learned section here you know conduct an ROI analysis before you're building out your own tool just because open source exists doesn't mean you should use it you know it would be great if you're using that to reduce your licensing costs but take into account you know the the cost of managing open source because there is a technical skill that's required for maintaining the thing take a look at your existing talent pool within your organization or even across your company exent yeah I mean to the point of open Source you know if you are an engineering team then you can work with the open source what we found was with open source you can submit your code upstream and it gets hardened so you can contribute you can customize the tool if you buy a third party CSP technology it's very difficult to influence their road map because we are one of the many customers that they have so we really like open source approach but depends on how large is your engineering team that Don talked about before jumping in into building this tool be it open source or proprietary licensed uh product getting the right Talent who knows what to do is was big learning um establishing the processes the RO getting people used to looking at their forecast and variances and all of that and then tool becomes an enabler rather than the end end goal standard cost was a big blessing for us it's removed so much of noise from our phop team it's just so intuitive to for people to see yeah I used an AMD instance I can see my cost go down and it's just so so intuitive cost allocation also be were very deliberate if the cost that I'm going to allocate if it is going to be less than 1% 2% it's not going to make any difference to the underlying data analytics why unnecessarily make it too complicated to allocate every single dollar so we hold them centrally like the elevator in the office building and optimize the algorith such that it is used effectively rather than allocating that peanut buttering it to everybody else deploy your platform close to the data gravity maybe you have a tool that you like but it only resides into one cloud service provider but most of your data resids on some some other uh platform you you don't want to be blindsided by unexpected data transfer fees and use Regional Services where possible Right architecture um that can scale the third party software tool especially for kubernetes that we used could not scale to our scale so the architecture that you design should not be for the current needs but for the future was a big learning for us and plan for scale don't become the bottleneck for your company and having other teams on board onto the finops platform allow multiple teams to be developing onto the platform simultaneously data governance um all this machine learning gen talk about uh in the industry everywhere at the heart of it is the data needs to be well organized to draw inferences from that so for us data governance and the pyramid that Dawn showed having those roles which curate the data so that you don't have pollution of the data is really crucial that we learn um Dimensions across csps so that you can conform and not be siloed in looking at one CSP cost and separate CSP CS every element if your databases are multi-az you should ask the other service provider to also provide that data so that's the probably in my view the next Focus 3.0 requirement it Focus one gets us up to here but our needs were much broader for us to save cost had we had to build something completely of our own so our finops journey is continuously evolving you saw that we started off our journey using package software and manage Solutions and then we since grow to require more tailored Solutions in order to meet workday's business needs we know that Ai and ml are continuing to evolve in this space um but we're going to continue to improve the user experience either by using AI or by uh developing more intuitive slack uh action-based Integrations or by you know integrating a more seamless data catalog solution so our fof platform is going to continue to evolve and improve and so is our practice and with that be happy to take any questions excellent big round of applause guys give him a nice big round of applause that was very good thank you who wants to kick us all right easy thank you for the great presentation congratulations on your journey um so I was intrigued by the standard thing that you mentioned there about what unit cost about the numerator and nominator how you're thinking about it I was intrigued by what you said on on customer so did you also standardize the size of the customer because I'm assuming the cost per customer the size of the customers will also significant ly vary size of based on what they're using let's say your work day like recruiting versus payroll and stuff like that right so all that would vary so how do you think about those different dimensions on product or the size of the customer yeah so the cost of public cloud is standardized right so the Delta between the actual cost because you can't hide away from actual just because you got a standard cost so every month we have to absorb the variance if anybody has done cost accounting for manufacturing these concepts are are well known for you know almost hundreds of years so the Delta it's more of a financial engineering part not so much or go to market impact right say let's say my cost for an r.x large instance internally will say 6 cents per hour actuals could be 5.9 Cent for 1 hour based on the ri coverage it could have been 6.5 at the end of the month all that Delta should be tiny so that your absorption is not too high thank you so much for your presentation and also congrats to accomplishing this um reading our own tool also knowing what is behind it a lot and uh my question was because we were talking about the account segmentation was it a decision you were driving because I mean it's a lot uh also uh accompanied by security decisions and whatnot is that something that you drove initially so saying you want to have that big uh isolation zones or part of segmentations within your clouds or yeah that's a great question so we are an HR software company pii all we hold pii data so as a result security is just everywhere in the company you can't you can't even breathe without talking about security so the the Mandate of many small accounts and not polluting One account having multiple different typ so the the account the organization structure the folder structure they inherit the policies and this stringent so tier zero will be only in these accounts um so every new project request there is a friction it's not like you can an engineer can create a new account just by themselves through a console it goes through this rigor to say does it require a what we call a TSR a Security review because you might be using some data which could be dangerous and in that friction and that workflow is also finops to say what are using an R&D is this account for R&D or is it for demo sales demo so that tagging of or not tagging it's a cost center assignment in the metadata gets done upfront so it is not after the fact that the account has got created and then we are chasing everybody to see what type of workloads are there in there all right who's got our next question for Don and denesh uh thanks uh just to follow up on that question so um if you have you know it's going to be an R&D account and you you obviously have a lot of infos seex I have a security limitations but do you all also apply limitations that are specific to finops in terms of costing you know do they make sure do you put limitations such as this account cannot create uh an Axl instance type of things like like that across the organization or you kind of look at it retroactively and as a quick response using your uh Opus tool yeah the the governance is only the type of workload not what instances they can use so they can use the most expensive instance but that's where our normally alerts come in to say I did not see this are you really sure um and then so we want to integrate with slack as of now it's just one of the the HB and spoke model that we talked about so the spoke for that particular subject area will go and work with the develops team to say I see this spike in usage um is just really intentional and then we look at the usage if it's a kubernetes workload then our dashboards automatically show your request was 2 gig but you are really using 15 MB you are oversized get your request correct yeah but there's no constraint on the agility for developer it's just that the upfront whether you can open an account or not because they they're trying out new things so we don't hold them to say can't use Lambda functions so you can't use this there are no restrictions after that I'm just curious about your your pyramid of different user types having different kinds of access uh did you find that the users required a lot of training to kind of get the hang of the level of access that they had as far as creating dashboards or creating their own queries or were the was the tooling pretty intuitive did people kind to get it pretty quickly yeah I I suppose uh you know that's a bit subjective but um we so look light Dash is an open source equivalent of Google looker and so uh we were using Google looker and we introduced light Dash and so it was a pretty natural transition for a lot of our users but then also those who are just coming in uh fresh did um did find it relatively intuitive we do have training material but uh the training section is is relatively small because there's just not so many options that let's say consumers have to be able to uh create their own dashboards it's pretty curated they still have a lot of features that they can um drill into but there's not so much that they can do that really allows them to shoot themselves in the foot thanks I appreciate the talk uh two questions one how big was the size of the team that uh um put this together and operates it and the second question is why do you hide your CSP discounts from your consumers maybe I'll take the second one the first one I need to think whether I can this in the recorded video uh but the second one was why did we hide the discounts yeah because you know we are multicloud and in this world there is no secret so when you're negotiating with one CSP how much crossover discounts or PPA you got you don't want to reveal your hand right so those are really confidential data point from my perspective and it helped us when we just signed a contract a few months ago um yeah we don't want to reveal that ever now how much how large is a team so there is a finops functional team it's just not pure phobs they are bi oriented let's say who understands you know how the star schema is and how should you think about so they are really product managers with strong they strong let's say 80% on phobs domain but they are also 20 25% on how bi systems work and we hired that kind of talent engineering Talent they have petabyte size worth of data experience managing our observability our stats Warehouse as we call it so it was a combination so the reason we were successful in a way because of that experience and that you know intersection of experience of not only fofs but bi to give really crisp product requirements to the engineering team so that they can go and build that all right who's next looks like that's it I think we might be all set thank you guys so much let's have another round of applause for Don and denesh from workday thank you guys thanks for watching check out more finops X 2024 content on our YouTube channel on the 2024 playlist support our Channel by liking subscribing clicking the notification Bell and by leaving comments and questions for our speakers we appreciate your support
```

## Transcript 3

```
hi everybody Welcome um my name is Peter Crenshaw I am a director of cloud shinoffs at ukg and here to tell a little bit of a story about our fin Ops journey and our specifically around our tooling and what we decided to do uh to start this story we got to go back about a year so I'll cover some of our history some of the concerns and complications that we had what we needed to do and how we achieved those so uh as everybody knows as you walk out the doors there's all these vendors out there um if you have finops in your title on LinkedIn you're probably getting bombarded with [Music] um yeah that's fun all right I can't walk and chew them at the same time so we'll see if I can do this with two hands um anyway just to recap my name is Peter Crenshaw I'm a director Cloud finops at ukg uh we've had a unique fin Ops Journey uh starting about a year ago uh so it's still a work in progress but wanted to walk through kind of our decision points some of the things we took into consideration uh you know you walk out the doors indexable hall there's a dozen plus vendors out there that can report on cloud spend so why do we choose to do our own uh what can you think about as you go down your fin Ops Journey regardless of where you are on tooling and whether or not it's worth it to invest in your own uh home built homegrown application or go the third party so without further ado there we go uh so I'll cover a little bit about what ukg is our background uh some of our unique cost challenges that we've run into some of the custom solutions that we've built to achieve the results that we needed uh initial wins so these are things that right out of the gate we we're able to say yes we succeeded in accomplishing this goal and then what's ahead so we have a large road map like I said this journey has only been about a year uh in the making but we've got a robust agenda that we want to get through and that's was all taken into account when we decided on a tool it wasn't a narrow focused hey let's look at just our Cloud spend we expanded that out across our Enterprise so a little bit about ukg in case you haven't heard of us uh we were created into in 2020 uh with the merger between Kronos Incorporated and Ultimate Software uh both big powerhouses in the HR payroll time and attendance space we provide Workforce Management Solutions that are cloud-based uh something of punching it on a Time Clock managers approving time time off um HR systems things like that we are dual headquartered because of our mergers so we have headquarters in Lowell Massachusetts as well as western Florida and we have a smathering of products uh they're across on-premise private and public clouds so we have a little bit of spend everywhere or a lot depending on how you look at it we are about 15 000 employees globally and growing just about every day and our public Cloud spend is north of 120 million across gcp Azure and AWS with a large percentage of that in gcp so we are a focused gcp shop on the product side as well as a corporate I.T and then we have through Acquisitions we've inherited some AWS environments we have some Azure environments as well so that's about us so what makes our situation unique so 12 months ago we were in a gcp reseller agreement uh we instead of going direct with the provider we went through a reseller for some additional discounting uh yes that saved us some money but it brought a whole plethora of other issues along with it um things like limited spend visibility without paying more money to the reseller so hey come save all this money with the reseller but we're also going to charge you for any of the tools to see what your spend is uh so we kind of a black black hole of data to your discounting so the reseller got a discount from the provider the reseller gave us a discount so each level was a different discount and at the end of it we couldn't see those different discount levels log in to certain portals those don't match our actual bill we couldn't reconcile anything because of the tier discounting and that caused misaligned data sets so again we couldn't match invoice to actual cost mixed cud methodologies so between the two companies one you shared Cuts one use Project based Cuts so they don't mesh you have to align one or the other and it was very hard to manage we were managing a very in very distinct separate views yet trying to offer a sweet experience so as our developers on one side of the application we're developing we're having to do one methodology for cud I think we'd have to switch and do something completely different across the two organizations we had 16 individual billing accounts across three gcp organizations so Access wise data wise it it was just a jumbled mess does anybody remember the game Pickup Sticks you used to drop everything on the floor and then try to pick them up one by one that's essentially how our environment was it's like Pickup Sticks we go look for a data set and it's it's over here oh you need that access to get over there different logins different organizations very very complex and then finopsis new our engineering teams don't know anything about Fin Ops my team was brand new we understand what phenops is but how do we teach the internal teams how do we work and educate our organization on what finops is different product topologies deployment structures tools and hosting Solutions across the entire portfolio so coming from two different organizations that merged into one company a has six different inroads into gcp company BS six different inroads now suddenly we have 12. how do you manage that how do you lock that down how do you label things Etc organization complexity so again two organizations two different structures two different ways of doing things company a Cloud supports everything from engineering to customer facing applications Company B Cloud supports only customer facing applications and Engineering does their own thing who's responsible for spend who tracks spend who does all of that and then an asset labeling deficiency so again two companies two different labeling strategies we're now one company how do we get to a point of seeing our spend across the entire ecosystem in one single pane of glass you do two different reports based on the labeling schemas do you manually try to put things together how do you make this work so what do we do so we did go through an RFP process with multiple industry providers of spend data many of them are out there in the in the expo hall and we found that a lot of them could not ingest third-party data it had to be direct from the consoles it had to be direct from the bigquery exports I'd be direct from AWS they could not handle something else coming in without extensive customizations we also started reviewing all of our system scenes were recently at one company what do we need to do can we pivot our business direction from this reseller agreement to make things better to make things more improved more streamlined so we decided to go direct with gcp so we assigned a multi-year direct agreement with Google Cloud that eliminated the third party reseller after a big migration effort and it gave us all of our data in one spot now we're not trying to chase it down we can combine all of our billing into into billing account it's that we have access to and we can actually work directly with the provider we incorporated looker as our business tool for fin Ops so with Google's partnership we worked through building out custom dashboarding based on our data and our data needs at the time as well as what we wanted to ingest in the future so being able to create new data sets being able to create new views more visualizations based on our needs at the time versus something pre-can at a third party would provide this includes history from the third party so this was a big piece for us because usually when you migrate it's kind of a break point you don't have any of your history it's only from this point forward you can record and then you you're still kind of stuck in the back in the past we were able to incorporate that third-party data so now we have a continuous data set we can we ingested 11 terabytes of cost billing data from our our third party into the looker solution so that we could have continuous reporting all the way through so now I don't have a gap I can set a date range that goes back two years and I'll get results in the same pane of glass as I do going forward because we went with looker which is owned by Google the cost of the tooling can counts towards our commit so it helps us break down our commit it helps count towards that commit and it's all under one package we don't have separate agreements anything like that we did migrate and consolidate all those billing accounts so we had more than 16 before we're down to six so a massive migration effort moving billing projects around to get them aligned into six standardized billing accounts that number's gone up a little bit because we've done some Acquisitions but we started with six we also transitioned that cud model so instead of managing cuds one way for one Legacy company and another way for another Legacy company we transitioned to a shared model within each billing account so now we are managing cuds a single way regardless of the billing account you're looking at so we're not doing different math we don't have different calculators the entire organization now understands how we manage cuts and it's not based on your reporting structure or the product that you're working on we also launched an executive sponsored Enterprise labeling initiative which is actually going Live this week impacting all of our public Cloud assets this is to help us get to the showback chargeback model so prior again we had two different labeling schemas not everything was labeled so it's very difficult to run reporting so we went to the top so our chief product officer sponsored this set a mandate and said everything in gcp needs to be labeled we have millions of assets out there that over the last six weeks we've been working through labeling the deadline's Friday so we'll see what percentage we hit by Friday but as of now we are starting to see the results of that you know 30 even 30 percent labeled we're able to start getting more um able to dissect our reporting a little bit better get more granular give teams the data that they're looking for so then we highlighted the value of that asset labeling with these teams before we launched the initiative so my team set up meetings with all of our engineering leads we have a value stream structure so our pillar and our value stream leads we all got on a call we walked through some demos of what we could do with labeling look if you have this label we can give you this data and kind of open their eyes to what is possible this helped us accelerate that labeling initiative that we just launched so by them seeing a demo and saying this is the data I want to see great I can give you that go label it that way and I'll give you that report helped us advance that everybody got on board everybody got very interested and of course they said when can I have it I said after you go label so don't put the cart before the horse but we did an education campaign worked with those leads they pushed that down and we are starting to see results of that effort so this is an example the numbers are are altered uh this is an example out of one of our service costs that shows the breakdown of the cost between our different types of spend you've got our Marketplace our snapshots Network Etc this is out of the looker-based visualization based on the toying it also gives us visibility into our value stream pillars again altered data but we're now able to give an executive view of the spend day by day week by week month by month and it shows which of our value streams is spending the most is that the one that should be spending the most where are the spikes where do things decrease and why so these teams are starting to be able to use this data just as they would you know any other reporting but we didn't have this visibility before wit now one thing I will point out these reports were created through the Custom Tool without the labeling in place that just is wrapping up this week so because we did a custom tool we were able to group projects together we were able to consolidate things into the groupings that we wanted to see without having every asset labeled whereas other tools require you to have labels in order to do this type of reporting uh our reporting is only going to get better now now that we are labeling at the asset level we didn't have to do the manual work of mapping projects and et cetera et cetera but this was all prior to that labeling initiative this week or last six weeks I should say we're also incorporating our cud management so we have customized cut management views that align with the gcp portal so between the gcp portal data and our looker data we're able to see our spend our usage where we fall we have a blue green release cycle so we'll see big spikes in our usage during those releases um reason I want to point that out is you go look at the Google recommender or any other third-party recommender it's going to say hey based on your spikes you should increase your cut commitments those spikes only happen during major releases four times a year so we have to wait for the cycle to continue until we can go back and look at recommender and Shrink our data period down to not include those spikes so we have to do some some funky filtering for the recommendations but this gives us point in time on one screen what we're using how we're using it in allows us to make those those educated guesses on what we should go with we can even break it down more granular so we can go by region we can go by project this shows us a very detailed view of where those cuts are being used how they're being deployed how they're being separated because they are shared across the entire billing account and the changes where where things are higher where things are lower so more granular more detailed than the console provides customized to our environment how we operate navigating around our release Cycles navigating around other things that would throw other metrics off so initial wins so this is one of the big areas that we were focused on again this wasn't just about Cloud spend we were looking at our entire Enterprise to see how we could help improve things how can we make processes better faster for the organization so click to refresh reporting versus manual data updates so now we go in we refresh our dashboards boom we have the view that we needed of our data we can schedule it we can send out daily weekly monthly whatever the Cadence may be uh but it's it's there at our fingertips ready to go customize allocation views for each of our billing invoices streamlined to our PO process which reduces dozens of manual accounting entries per month to allocate funds or for chargeback so even though we don't have a full chargeback model some cost centers were getting charged a portion of our spend like we have an environment for our internal teams to use there's four teams that share that environment they split the cost of that every month that's four different chargebacks that we we do in the old process we paid a single amount and then our accounting team would go in later put in journal entries to map those chargebacks back to the appropriate cost center I don't know about your accounting system but when we put journal entries in we lose all the history there's no link between the transactions it's just just like writing in your checkbook right there's no notes it's just transfer from A to B so at the end of the quarter end of the year we'd go and look at the reporting and those cost centers just had dozens of journal entries for their cost did not know if it was gcp cost didn't know if it was some other cost that was cross-charged so we wanted to try to reduce that we wanted to try to get ahead of this to not only clean up our accounting reporting not our finished reporting our accounting reporting as well as reduce the manual effort it was taking to break those costs down po data compilation and submission process time reduced from weeks to hours so in this instance we used to take all of our data into Excel process it split it up amongst those internal categories try to get some idea of what it was send it off to our business operations team that creates a PO to pay the invoice that process took weeks because we're grabbing data from different sources we didn't have details from our third party there was there's a lot of different reasons why now we've actually built a customized dashboard to give us that allocation for that those new invoices at the click of a button so when those new invoices come out we go refresh for the last invoice month the data refreshes and it's already been split out based on the logic behind the scenes that reduces that whole process from weeks to hours so now we can get that invoice into the PO process within the first week it's issued versus waiting until the end of the month well we can finally get it all put together an increased visibility into our biggest areas to spend before we just got an invoice with a massive amount due what do we do now we can actually break it apart show teams what they're spending which cost centers should be charged and then customize spend breakdowns by service so which service is costing us X which is y so this is an example of one of our Google invoices for one of our billing accounts 1.6 million dollars it's a single line item on the invoice that's all we have to go off of so we people are talking about dozens and hundreds of lines on their billing invoices this is what ours looks like a single line so send that to AP and say hey go pay this they go what is it I don't know that's the problem we're trying to solve with one click through the custom solution we now export a view that looks like this broken down by line item shows the different amounts that make up that invoice we've even included if there's an existing po to pay a portion of it for like a Marketplace provider Etc we've included that put in the PO number and then all of our accounting coding so our our company codes our account numbers our cost center numbers all the way down to you know the finite details that a county needs what does this give us when business operations now receives this file in addition to the invoice because they have to match them they now have a breakdown for their po when they put it in on the PO side versus a journal entry on the back inside you see all of the history all of those payments are now made it's still a single payment but all the tracking and the logging in the accounting system is done up front there's no manual journal entries because it's all done on the PO side so our accounting team is much happier because they don't have to have the staff to do all these manual journal entries they have the historical reporting to see that it's actual gcp cost or Marketplace cost or whatever it might be versus just a journal entry of a transfer so this is drastically increased the visibility increase the visibility and the top turnaround time for getting our invoices paid the accounting team the FP and a team are very very happy with this as it is today and this grows so this changes so it's not a set list as we add new Marketplace items as we expand out and start doing more chargebacks we want to break things out more granular this list will grow it'll soon it'll soon be dozens if not you know um hundreds of lines for that PL so what's ahead so we've got some quick wins we've done a lot but what else do we want to do with this custom solution so we want to get to an automated value stream specific reporting based on our new label requirements again that's all happening this week so all these teams are Marching towards this deadline of Friday we're switching our reporting from that customized view that we did prior to labeling at the asset level two asset label reporting we'll build out those reports have them sent every week those value stream and pillar owners will see their spend every week be able to start diving into it and then we'll customize it from there as as their needs come up Trend reporting and then we're also looking to add additional data feeds to support other businesses or other units within the organization these include third-party tool costs whether it's on-prem whether it's in the cloud we can import that cost data manipulate it put it into the same views and break it down the same way private Cloud costs we have a bunch of data centers still we still have co-locations we have costs associated with those we want to start pulling that data in to do full cost analysis across our ecosystem our sales data unit you need economics we want to get to how much does this customer cost us to run or how much you know peplum essentially per employee are we paying for these different Services we want to get to that but we need our sales data in order to support it capacity data what does our capacity look like where are we increasing where are we decreasing where the cost implications there and then Revenue data to get back to those unit economics also streamlining our payment processes with Google we're trying to get to a more standard we got that visibility there we have the internal PO process there there's some other pieces that we want to work through uh logistically mainly uh just to streamline that all the way through so that those invoices come in they get processed and they're out hopefully within the same month or two and then continued education campaigns for all of our resources as more Engineers hear about labeling as they hear they're why are we labeling well because we need fin Ops reporting well what's finops what is this we're educating we're working through the campaigns trying to get more teams on board yep no you're good absolutely all right pictures are better at the presentation so all right so that's what I want to cover do we have any questions all right and I have this fun thing that I get to throw at your head I mean at you to catch to ask a question you ready do you have a question all right we do have a couple minutes so I could walk it to you but I'd rather throw it is that okay as long as I catch it um so one of the things that we're doing as well is figuring out how to manage the marketplace spend specifically within gcp yep so we prepay most of those does this reporting that you have take that pre-mayment and amortize it so that when you get that one-time hit it amortizes the cost over the life cycle of the contractor you guys still working through the logistics of that we are doing that on the accounting side got it we're paying it up front so this report shows that The Upfront payments because it's actually what gets paid on the invoice it's the PO and in the accounting system picks it up and says okay this is a three-year five-year amortization all right I have to go all the way over here so I'm going to throw it are you come on play along hey uh a lot of what you talked about was around like billing billing uh or chargebacks or that sort of thing do you have for your custom reporting do you have um better like cost optimization in insights and things like that to help teams with actually spending less yeah not yet it's on the road map we want to get there um it's not built out yet but we are looking to get those recommendations into the same tools so that it's a single source for all of our all of our recording we still have a console obviously but we do want to pull that in all right okay I'm gonna go you glasses got this [Music] yeah you would um you talked about having this goal of uh doing the sort of labeling and having a goal of Friday for millions of entries is that uh it's not an effort that you're able to kind of do at scale or how is that happening uh very chaotically um so yeah so we created it took us months but we came up with a new labeling standard with a couple mandatory fields to help break things apart by our value stream model and then we push that out to the teams and said every asset has to get labeled so our chief product officer sent the email out saying June 30th is your deadline get everything labeled everybody sat on it for four weeks and now suddenly they're like hey June 30th is coming up am I supposed to do something yeah you've got a million assets go label go um so it's it's very chopped up we're running into ownership issues we're running into you know well my team deploys it but somebody else uses it so who should be labeling it so we're fighting through it it's not easy uh but at this is the first time we've done it at scale at the ecosystem level so we're definitely running into challenges it's not as easy and we can't automate it because there's some projects that have multiple value streams supported within them based on the structure and how they were built so automation to say hey anything in this project label is this value stream won't work it'll mislabel it so it's a manual ad for today hopefully going forward it's easier as we go but even this week we're seeing new stuff spun up without labels so having to go back and keep it's a fight yeah um so you mentioned that the primary reason for building versus buying as opposed to going with somebody outside there the third party service providers was because you had a specific business case that didn't necessarily fit with the generalized format that they have out there I'm wondering was there any consideration in terms of how much it would cost to build something like that as opposed to the fees that you would be paying to one of those service providers yeah uh so a lot of the service providers are a percentage of your spend whereas by investing in the licensing for the tool and actually paying for the tool that cost came in much lower so instead of you know a percent of spend it's a set rate we kept our licenses low for the tool so my team and two developers on our SRE organization are the only ones with access we're not planning on opening that up to the entire organization because that means your license fees go up by keeping it small to a concentrated team that can then Supply those automated reports we're able to control our finops costs versus a percentage or I spent all right okay so there's a lot of questions left we are at the end of time but the good news is Peter is gracious enough to stick around for a few minutes here to answer after we take all of these microphones off of them because we I kind of wanted to throw this at you just to see if you could have one more microphone on you but your hands were full yeah so we won't so he will stick around um again everybody this is great I could tell that all of you like to uh by all the pictures that you were taking on your phones and your tablet so awesome so one big round of applause for Peter Crenshaw at ukg thank you so much Peter thanks for watching that session I'm sitting here in San Diego right after phenob sex we hope you join us next year here live 2024. in the meantime please subscribe to the channel and join the community get involved join the Summits get in a working group and don't forget to get fin op certified because next year here in San Diego for finopsex it's going to be twice as big come join the party come meet your people welcome home
```


## Transcript 4
```
we've actually asked one of them from Fidelity to share today a bit about how they're doing scorecarding and using some of those kpis and metrics to measure internally how different groups are doing so I definitely want to welcome Zach seitham who sits on our governing board and the no Crowley who also is in one of those technical advisory Council seats that we mentioned to share a bit about what they're doing there Zach Knoll thanks Dior first off apologies Zach I pulled away last minute um so I wasn't going to be here today so unfortunately uh you're a stuck with my voice through this um so apologies for that um so uh so let me I wanted to talk today about the Fidelity Cloud Journey first of all for anyone who's never heard of fidelity we're a financial services company privately owned by the Johnson family we predominantly do personal investment stock plan services and 401ks we do those I think pretty well um and we've been doing those in our entirety uh for the last 17 years though that I've been with Fidelity uh where I've always thought to be more a technology company and even to quote our former chairman we're a technology company that does Financial Services in 2016 we started our journey to the cloud we deployed our first application out there we launched our technology strategy to be in multi-cloud environment as well as planning to have 70 of our of our applications in the public Cloud by the end of 2024. um we are Zach and myself we work for centralized I.T and as part of that at the Enterprise cloud computing business unit we hold ourselves to tree pillars to safely and securely adopt the cloud at scale to do no harm to our brand and to adopt the cloud in the cast effective manner so as we started to migrate to the cloud the next slide there please um we we created our Fidelity Finance Focus right uh for us and for finops we started to migration we created our final Ops practice uh collectively uh we held response we we took responsibility for achieving a value for our Cloud spend right as I said earlier there about adopting the cloud and the cost effective manner so when we created these this Focus these Focus areas for ourselves we wanted to have cross-transparency to make our costs visible Financial controls have checks and balances between technology and financial orgs have purchasing strategy so we decentralized finops team RIT optimization around automation for developers or Engineers uh improve utilization when they were when they were using compute and most importantly to get the best value for every dollar we spent in the cloud But ultimately at all this we wanted to create a culture of accountability so we had to kind of look at what tooling and what information how we get that out there for people around dashboards and kpis and stuff like that so with that we started to do we we created our internet our first International next layer please uh we created our interplay our highway to the cloud as we call it internally here this is a first Port of Call for all Technologies for everybody in Fidelity um as as they start their their Cloud Journey it provides the information on the different cloud service provider platforms and products and it makes it easy for application teams to deploy their workloads to the cloud so information there for fit Ops predominantly as well as everything else that we provide from from Central I.T so we've got governance and networking and everything else it's just an internal site to get information out there um when we when we have this site and we start to transition them out there to Native tooling from our providers so both uh depending if they want to utilize uh sorry depending on if they're going to have their application to be born in the cloud out there if they're going to migrate an existing application out there depending on which uh which provider they use they may be using the AWS cost management and trusted advisor tools they may be uh to be encouraged to use that if they're using predominantly AWS applications it's out of the box it's day one information that gets them there and it gets them and understanding their costs sorry next slide again please um as well as that then there's also the Azure cost management and advisor tool these are there as I say these are tools that are native straight away for people to use with documentation how to use them and understanding on them could you go on again please sure and then so as I said we have these tools out there to get the visibility for users to developers and everything else to get out there and get their information as well as that we had the opportunity the decision to make to go do we do we develop or do we go and use a third party tool to get really into the nitty-gritty and the details of the spend um at the time we decided we would use a vendor tool um it gives us it gives us great great visibility into our cost like business unit by account by environment um as I said we're a multi cloud provider organization so this gives us just basically a single pane of glass to get that data of the spend outdoor to our tour to our toward application development teams um we have an extremely good tagging policy in place so we can basically get the cost right back to the penny for for these teams using these third-party tools uh and and and it saved us having to go initially to develop that right um it helps us to make the information and the views that we present to people it's actionable to them it's exactly what they're doing and helps them to to understand how they're spending it but some things with these like we kind of went that there's not one tool that does everything for everybody so we have also gone and we've developed tools ourselves as well as dashboards and we have we have tools like notifiers we call it which basically is an in-house tool that we have developed that pulls data from our various different tools that uh applications that we're using and it sends the information via email or instant message to the person who can take an appropriate action and and and that is useful to them as I say as shown here we also have our kpis which we use to Bubble Up our information but to a business unit level and this this helps us to to to um sorry this helps us with the uh with the information and making it it better for them right so we have we have our tree pillars of our kpis which shows slightly different information for everybody um therefore our cios and their senior leadership teams as we're as we're mature in our practice we were looking at other tools that were out there so we've got obser availability we've got security tools that were purchased for the firm for different reasons they were never brought in for phenops they weren't there originally but predominantly for that at all so as we looked at them we went are they able to provide us data that can give us uh cost avoidance and saving opportunities um I have on the screen there I've got some sample Bots we use our security tool that that goes out there and we use our Bots these Bots help us to provide hygiene and clean up the the the the the the the the cloud space after our developers have been there we want them using the Innovation the cloud provides of them so if we can automate stuff to to make their life easier we will try and do that um we know that this is never ending and these are just a sample like we've got Lumber's act which is top left one there he just goes out there and he turns off your ec2 instances at the weekend we've got Oscar who goes out during cleans up the trash if you've got nowhere to send your traffic from a load balancer we'll delete it for you we've got our volume cleanups our snapshot removals and there's information up there from previous working groups on finnoffs.org as well about that about that one as well so we've got observe availability tools as well that helps us to look at utilization of our existing compute stuff that is beyond what we're getting natively out of our tooling out of out of the cloud service providers it gives us that information to help us to look at our kubernetes and get more information for people to be able to take actions in a new life so we also have our to come back again to my kpi dashboards which is where we bring all of this data into it's on the next slide there please we bring it all in from all of these different ops availability tools are are our security tools um and as well as our cost management tools we bring it all in we got a right sizing recommendations from from third parties from C from the cloud provider native tools as well as I specialize specialized tools for kubernetes and things like that um when we bring all that in we kind of go where is that data coming from is it coming from cloud watch is it coming from the third-party agents and how can we utilize that how can we build it up so as I said we have our tree pillars there for our for our for for our kpi but below each of those pillars we've got supporting metrics so we've got more information that allows our teams to drive down to again pulling it all together allowing them to then dig in and go further and further into the information um I have there as I said a snapshot of my business unit score of the business unit scores and if I go to the next tool there as well we take that and we drive it even further back down and get into our senior leadership team right and we create a leaderboard of senior leaders I've just got a black note there just at the names and stuff like that but what that shows us is we create this whole or this wall of fame as opposed to you know traditionally create a leaderboard and you're trying to shame the guys that want to get the top what we actually try to do here is to show the success that is being done by our Engineers buyer buyer buyer by by the people using the cloud how they are making things better um with the information we're able to get them right so as we matured our practice we want to do more with all the data that these different tools can give us so we try and pull this data out and make it back visible to to people um so in in closing I guess uh we we're still on a Learning Journey as we use these tools um we have we have as this about 17 000 technologists working about 11 000 applications with infidelity and we know it's going to be difficult to get the developer Community to move on any one tool so what we try to do is we have our takeaways um which is when we look at this stuff we have making our fit Ops data visible right which is you know uh one of the the principles of fin Ops anyway and Ops Foundation I should say uh you know you must know what your bill is you must know what you were consuming um we are always trying to say looking at the tools we have and when I think of that one use the tools that are there so we never thought about our obserability tools or or security tools has been able to help us from phenops point of view at the start and we probably would never have gone and purchased those tools to do phenob stuff but they give us information that is useful to us and helps us there's always new tools coming there's always new people trying to sell you stuff but sometimes maybe you've got something that's there as well that you can use that's there so can you take the data from those tools make it actionable and meaningful to your to your end users and then what we're trying to do is put the engineer our party put the data where our Engineers are and we create their culture a better culture of accountability for them um if everyone is responsible for their spend can we can we get information out to where it's to them they may not always be able to come to my dashboard they maybe they have a load of dashboards already that they're being sent to look at so what we're trying to do is putting that information out there again developing tools ourselves that will do that for us again as a Notifier tool to get that information back out to the edge um so that's uh that's our tooling that is amazing all the comments in the chat have been blowing up I've been collecting a bunch of questions that folks have for you um based on time we're going to need to move forward but we do have the breakouts at the end so I'm hoping you can stay around for a bit to answer questions at that point because people people want to know a lot about this um so thank you so much for sharing the overview no problem thank you right Jr here from the finops foundation thank you for watching please go to finops.org if you want to get plugged into this amazing community and of course hit subscribe right here on YouTube to get all the future content hope to see you soon
```

## Transcript 5
```
uh am is going to talk about the total cost of ownership of applications and how to calculate them uh certainly when you're need to keep into account contract structures discounts and so on and all of these challenges and amid is going to talk about this so take it away amid awesome thank you thank you everyone for being here I know that after 2 o'cl lunch the pool is right there so appreciate everyone coming up uh my name is Amit and I run uh public Cloud fops at City Bank and as part of that team um as an intro I figured I'll tell a story of how I got into uh finops and hopefully this story resonates with a lot of you uh previously I was at a bank building out a credit card and this credit card was going to be on public Cloud uh it was going to be infinitely scalable we could build it really quickly all the you know trappings of AWS or gcp and as part of that we wanted to build out many environments right we wanted our Dev uat production then we also wanted prod parallel um performance environment a Dev 2 in Dev one goes down I guess whatever because again it's infinitely scalable we can keep building doesn't matter and then one day you know we get the call and I know everyone knows what I'm call I'm talking about which is from the CFO says what are you guys doing it's costing too much money for you to do something that's not even in production yet right and so then we said okay let's go take a look and you know have to Google where do I look at Cloud cost okay log into the ad of us account don't have permissions okay get get them a week later and we log in and I expect you to see C costs that were like Dev X cost uat y cost so on and so forth and instead what I see is like ec2 x million Splunk or rather you know any other services like you know Kinesis y million and then making heads and tails of that of understanding well where is this cost coming from what do we do right in this instance is really difficult and then also we didn't really have any sort of um SAS companies that were ingested into cost Explorer either right so if you use mongod to be Outlet well you have to log in somewhere else and the same problem you know you have a multi-tenant cluster can't really tell what exactly is the cost there and so when I joined City Bank I joined the public Cloud team and we were building out a platform which would enable uh um applications within City to go to the public Cloud without doing a lot of the trappings that you know we wanted to do one time things like security sdlc um observability and finops is one of those things so when I joined I wanted to be on that team because I remember feeling so helpless in terms of not even knowing Cloud costs where to look for but then be even how to optimize them so as an organization I'm in today it sits in the CIO office and as part of the public Cloud organization so the Mandate we have is not only the standard things like you know um showback cost optimization but it's also building tooling for all the applications within City for application owners to be able to know now use self-service to be able to handle things like understanding their cost right understanding chargebacks um and making sure that these chargebacks are not you know this might be unique to City we charge back all our costs back to different lines of businesses and applications and so what we can't do is say hey your showback is cost is you know 100 bucks and then at the month end we charge you 20000 because our application owners are relying on this data to be able to understand are they you know doing what they expected was there an anomaly so on and so forth there so the responsibilities of our shared team uh is specifically around you know uh show back charge back cost optimizations and then also centrally purchasing commitments right uh savings plans RIS so on and so forth there and so let's talk about single pain of glass right um how many of you guys have some version of single pan of glass today great a lot of you that's great so when I think about a single pan of glass view I really put myself back in the shoes of as an application owner I want to be able to see my cost in a way that's unified right it's not just AWS cost it's not just gcp but if my cost of an application has you know a mongodb Atlas store I have snowflake for my data analysis I use AI for my CDN well that's my application it's not bifurcated so why can I see my cost in a unified view for that right I also expect that you know cost should be standardized terminology and metrics Focus has really helped with that um there's a lot of good sessions happening today around Focus um it should be Timely right there's no point in getting data once a month if I knew this data to figure out anomaly section right it should be accurate which you know goes without saying no one wants to get an email that says hey we fat fingered a zero your cost is actually 3 million not 30 million right and the business context is where is an interesting one um and this one is really around you know the cost itself alone is not a good barometer because what if the revenue went really high and the cost went up a little bit right and the last one is explainable I don't think anyone here wants an email that says explain to me this cost and you open up your Excel spreadsheet and there's like a rough data error and then you're freaking out cuz MD is freaking out there right so sounds pretty easy right like it's pretty standard things we're describing and it should be really really easy to build this in a couple of months or you know you go out to the vendor Expo I'm sure some vendor is going to say oh we can do all of those things for you just give us you know some percentage right and so what I want to do is almost like unpack this wish list of things that that I considered to be a SAS or you know single pain of glass View and what I'll do is peel back the curtain and this image itself is generated by chat GPT um clearly there's no such thing as codal spending but you know there's still AI work to be done there um so looking at one at a time right a unified view of csps and SAS right the questions that often come up is is it build is it buy or is it buy an argument and often times the reason is because a lot of uh SAS vendors will say oh yeah we can can offer you AWS gcp and Azure and everything else figured out right and it doesn't really help when your uh application owner says well I went to your portal and it said I owed you $100,000 last month you charged me $90 then you have to go explain well there's an as risk you have to actually include you know your mongod DB charges afterward so it doesn't really help right um around Focus well what about SAS right we have Focus has done a really good job around things like you know what is considered an application what is considered metrics but the SAS is still the wild west right and I think Jr mentioned that this is something that's up and coming but oftentimes what I've seen at SAS vendors what they'll do is they'll have you know a vendor X dollar or buck or credit or whatever and often times it's not one to one which you expect it's that you know something's five credits $5 to a credit but then you apply your PPA discount so it's actually 430 unless you're in this region then it's 390 and you get everything in a credit right so trying to explain to a tenant that hey your cost is actually X credits and you have to carry the one to figure out your cost is not great um around timeliness um I think having work in observability or Sr uh areas you kind of expect the data to come as soon as they have it right you don't wait for the next day to figure out your application went down you get a PID your duty alert immediately right but in you know in the world of finops what happens is each vendor kind of decides on their own right you have some that are t plus 2 hours T being the time that you know a dollar was spent find out 2 hours later some of them might be T plus1 day some of them are T plus1 month you know you just get invoice and they say this is your invoice figure it out and some of them are yearly right where you sign a contract up front and all you're getting is some pecem showback data so it doesn't really give you the single unified view that you're expecting if all these things are occurring right around um accuracy this is one of my favorite ones right the the you go to cost Explorer say you see $100 right then you go to your uh SAS vendor that you're using and it shows $87 because your PPA got applied there and then when I give you a bill it's $19 because I includeed share cost along with Direct Connect cost so on and so forth so as an end user you're kind of like you kind of lose value in a number you don't trust anymore because it's always an asteris associated with it right um around business context aware and this is not a philosophical question but like what is an application right is an application imagine a platform that multiple different lines of business are using is that one app or is that each individual version of that app is its own app what about an application that runs across Devon prod but sometimes you have an ephemeral environment for you know testing but that's a shared cost right so really defining that out and adding that context is not as easy as it sounds and the last one is explainability some of the challenges we've seen is uh first mover disadvantages right we sign a contract with a new vendor we have the first team who really wants to use it and when they start using it they get hit with all the charges and they're like why am I getting all the shared charges I was expecting and you have to explain well you're the first one on it and hopefully other people join but today you're going to eat the full bill right and the other one is allocation of Enterprise costs right you have these costs that are like the support charges that comes as a on line item or the direct connects between your data center and the Colo right so they all keep adding up and so the question becomes okay it's not easy but it still sounds solvable right so what can we do I mean we could do nothing and just put our hands up and say look it's the way it is uh one day some vendor will solve it and if phop sex will meet them and we'll you know buy them and they'll take care of it uh the alternative is um you can do it yourself right and that's what we decided that um again chat GPD photo uh we decided that a lot of the problems I'm describing are data problems right they're not problems with optimization they're not problems with um you know allocations or something these are problems with the actual data coming in and the fundamental problem is that the dollar that we say someone spent is not fully loaded yet right there's still the support charge there's still allocations there's still X Y and Z adding so we decided that we can almost like pre-treat our data right in a way that by the time it hits a golden Source we can do forecasting we can do anomaly detection and when a application owner logs in and they see their cost they have full confidence that the charge that I'm showing today is a charge they'll be getting at the year at the month end right when we actually recover those charges from the different teams there and so what I want to do is kind of walk through three specific examples of this data activity that we've done and almost like the the recipes of how we go about solving these kind of problems right they're not exhaustive and you know I want to emphasize these are problems that we may face at city right but we don't we we may not necessarily face them at a much smaller company when you're really worried about optimization you don't really care about chargebacks or you only do showb backs and chargeback is a centralized function so it may not really apply okay so the first one I want to talk through is uh contracts with usage commitments right and the scenario is uh you know you must purchase credits from a SAS Fender UPF front and to in order to do this at a place like City we're a platform of platforms and so we go to these different application owners and we ask them um what are you thinking for in terms of forecast of using credits wise for the next year right and so they come up with estimates we add them up and then we say okay we're going to purchase a contract here now sometimes if we're lucky and we have good sourcing Department we can roll over unused credits to the next year's contract right but that's not a given so we kind of need to and the other thing is that sourcing we have to get commitments upfront from teams the reason for that is what we can have happen is team says yeah we'll use a million credits next year don't worry about it just get the contract and then year goes by and their Dev team decided to go a different way and now my team is left with the contract that's just unused and at City at least we cannot be positive or negative you have to be flat and with that being said is uh we have to get financial internal commitments up front right from these teams here and so the way to solve this right is what we've done is instead of saying like as a team look the credits that you purchase say team a purchases 120,000 of credits so 10,000 a month for a year what we don't want to do is say look no matter what we're just going to show you $10,000 as a line item who cares what you're using it doesn't make a difference you're already paying for it because we still want the showback and the optimization capabilities and the understanding hey we're using this stuff more and more every month we should think about why or if you're doing a rampup period what we don't want to do is say we just charge you 10,000 a month and you lose the context of well when we were in Dev we were only spending a th000 but now we're in uat we're spending 4,000 why did that happen you lose that right and so what we've done is effectively the solution we've come up with is uh we allow teams to carry over cost right and what this really means is that let's take my 10,000 example right a team has 120,000 commit for the year in credits 10,000 month and in month one they know that they're going to go to Dev and by month 12 they'll be running Dev uat production so they'll be far exceeding the 10,000 a month of allocating for them right so what we do is we still advertise that 120k divided by 12 right every month you pay your 10,000 that's a line item but for the unused portion right say in month one you use $1,000 only because you know Dev people are on vacation there are finops X whatever uh we keep track of that existing 9,000 that you didn't use and we will put it into the carryover balance for them right programmatically not like on manual or anything and what we do then is we say we know your uh you have this carryover amount Because by the time you're in production you might be running 30,000 credits a month but that's okay we have we we've been collecting the savings for you and the nice thing is this way is it also gets a sourcing Department the cash flow back right we still have to recover the full amount of the contract in a year and we're not relying on a team that says for example the alternative to this might be we'll only charge you the usage amount right it's pay as you go the burnd down so if a team use only a th000 and we only recover a th000 what'll end up happening is by year end if they only end up using 20% of their allocation they're going to get a sticker shock that says oh yeah the remaining portion of your bill is $8,000 thank you so much for paying the every single month and all of a sudden they get hit with this bill and then I get a call and then I have to explain well this is why it happens right um and then if they go over their usage this is where it gets interesting right we'll burn down from the 10,000 first will go into their credit pool that they've amassed over time and then they still get charged only 10,000 and the credits are affected right and then the use case of what I'm describing in production when you're running you've exhausted your carryover in any given month what we'll do is we'll give you the charge for 10,000 and any overage for that given month alone what we don't want to do is start exhausting your credits from the future for today's cost because we want you to know that hey you're actually running more than you had predicted right so you still get your usage data uh in a normalized fashion and this will be considered an advertised cost and then there's an overage line that which explains to you why this happened and this will also help us with planning centrally that we're running low on credits we should reup and then we already have the uh the amount recovered from these teams that are going over there um the second example I have is contracts with fixed charges and this is one of my favorites the example is you know SAS contract for large Enterprises often is not just pay as you go right it's not the hey use a credit you know we charge a dollar there's also these things right there's like the mandatory training credits who's for mandatory for why is it $100,000 we don't know but or my favorite one which is the shiny metal support right the Platinum tier quality support tier that which your company needs because you're an Enterprise and you can't remember the last time you open a ticket of them right so the question becomes well how do you split this cost it's a fixed cost it's almost the cost of doing business at an Enterprise and what we want to make sure we do is we don't want to penalize the teams that were first if a team joins and there's two teams and the cost is again 100,000 a month for these fixed costs and we get their commitment we sign a contract if a team joins later on we don't want to say well don't worry about those fixed costs those other guys already got it you guys are good right it's not really fair and we we don't also want to do is say hey almost like an insurance plan in health sorry this contract's locked wait for next year we don't want to disable teams from using vendors because of an issue with billing it's not really uh it inhibits uh growth there so what do we do um we decided that instead of doing cost follows cost that we're going to charge uh we're going to divide charges by n teams evenly but reassess n every single month so on the first month if there's two teams well 50/50 if it's month five and there's five teams now well it's going to be 20% each and this way you don't have this first mover disadvantage happening and what we're able to do do then is it's an alternative to this concept of cost follows cost right and the reason we did that is that it's not really f that if a team is the big elephant on this vendor platform but they're super efficient right they they optimize things all the time but they still get hit with this massive bill every month because we say well you guys are the biggest spender so you're going to get this bill because they can't control it they can't do anything about it except for look at it and say well it's a fixed cost why am I why am I being burdened with no one else on the platform and so we tell the teams like it's a cost of doing business uh it is the shared platform fee for City Bank to use this vendor and you just have to allocate it in your uh for and everything there um and then yeah the other thing it does is prevents a first mover disadvantage right you're able to now have teams join throughout the life cycle of the contract and as more teams join the cost of the shared is B the burden is reduced for every team starting that month forward um the last thing it does is yeah it adopts the throughout the contract so what we're not doing is limiting teams from sitting there and saying I guess we got to wait till December of next year to be able to use something that would be really beneficial to our team but because the contract work didn't work out there right uh the last example I have and if anyone was in New York for the Road show I did a deep dive into this topic and this one is around commitment based discount sharing and this one is specific to AWS but it could really apply to anything right and the scenario is uh we have teams within City that are using very specific instances of uh imagine an ec2 instance right uh imagine like the the AI team wants to use a p instances they're $99 an hour and we want to make sure that this team gets a uh Savings Plan Credit on their account first and the way you do that is you purchase at the linked account level and not the payer level and the way it'll work is that the priority every hour is given to any eligible spend in that hour in that account and if there's no eligibility then we go to the rest of the organization right sounds pretty straightforward and so we have use cases right it's a bank that we run applications 24 by5 Monday through Friday it's a market uh risk assessment and on the weekends we have 48 hours of downtime and so we scale down right we use kubernetes we're able to bring down nodes to zero everything's great but we know that the savings plan in that hour is going to get charged right every hour unless there's an eligible spend so what ends up happening is the purchasing team agrees they know that the 48 hour dip they'll still come out ahead right even with 48 Hours of dead time they're still coming out ahead ahead of um uh On Demand right just because the math works out it might not be the 37% they were promised but it might be 14% 15% right and then what ends up happening on the other side as a byproduct of this is that there'll be a team that comes in on Monday morning says look Cloud costs are so low on the weekends we did it they go to lunch Pat themselves on the back tell their MD we did it and they didn't really do anything right the Savings Plan engine decided almost like a luck factor of saying this account should get the savings plan today right imagine there's five different teams using the same product in that hour it's random or maybe it's not random maybe someone from us can tell me how it works um but at the end of the day what ends up happening is we have these uneven charges on both sides right you have a team that's paying for the Savings Plan every hour and another team on the cash cost it show zero and then they they don't really know why and we don't really you know explain to them why without having to like send an email out and explain to them hey don't put an anomaly on that that actually is normal and so what we did there was again we realized that what we can do here is we can adjust these charges we can look at the charges and understand that in that given hour an Savings Plan was used by someone who doesn't own that Savings Plan and the way to adjust them what we don't want to do is charge them list price right if the if the say if the on demand price is 20 bucks we don't want to just charge them 20 bucks they still got some benefit it shouldn't be zero but it should be something and so what we end up doing is we pay charge them the discounted rate instead for that hour we inject a line item for that hour and we make it very clear that it's not just ec2 cost this is a credit adjustment that we're making we're charging you and the story flips from why are you charging me excess to we're giving you a credit because you got lucky but you should think about getting your own savings plan or using one of our shared savings plans we may have at the pair level for the specific uh instance and region there and then we take that money and we give it to the SP owner itself right what we can't do is take that money and say you know my team gets to now have pizza parties every every day we have to give this money uh back to the owner but we explain to them this is not just uh you know an E2 rebate this is specifically because you got lucky that this weekend there was someone who wanted to use that that instance but it may not happen all the time and they should really focus on bin packing better right are there calculations they could do this weekend instead of waitting till Monday morning right and so while they're doing that though we're still getting the benefit to City Bank of you know the linked account sharing and we're not penalizing the team that uh was supposed to only expected 17% we give them overall net higher savings there and then the benefit of the recipient is that you know we tell them look look you got lucky you know how you could get this luck all the time purchase your own Savings Plan you don't have to rely on luck you can just purchase it or again use one of the shared uh savings PLS we may have now uh before I get into what's next I do want to highlight these three examples are really just examples of some of the data quality issues that are not inherent because of a CSP or a SAS vendor this is just Enterprise stuff right and we could we don't instead of relying on a vendor to come in and say oh yeah we can adjust this we can fix that for you a lot of this stuff is effectively this data quality that you can manage on your own and because my team is engineering and product we essentially look at the ETL as a way of saying before the data hits a place where we start doing forecasting anomalies we want to make sure we adjust these not only so those forecasts are accurate but our Downstream teams aren't expecting hey your application showed me $100 but you're charging me the $10 right or your application said I was pay as you go but then you give me this really massive charge up front we're trying to make it that every single day you can kind of expect that the cost you're seeing is what you're going to get charged at the end of the year um so what's next uh for me the my favorite slide or picture from finops is a circle right often times when you think about development you think it's start finish end but it's really not in finops right like now in the last year A lot of people are concerned about things like multi- tendency and multi tendency in kubernetes is a problem that's existed but now what about when you use something like vertex AI right you have one project uh there's no uh tag ability on vertex AI it's an API call you're making but there's a guy in your building who's just sending 50,000 page PDFs all day right so you can't just even split that cost it's not really fair so how do we manage that same with unit cost right um at City scale everything we do has to be scaled out without a human involved in it because what we can happen is that you know I work with a team that says we are simp simple API we have a database layer we you know we have a reddis cache and we have something in the front can you help us figure out what the unit cost will be we can do that but my team cannot keep growing to support that so what I'm really looking at is how do we build a pattern if you're an API based approach uh you should have resource tags obviously but then you should also have um imagine a Content header which includes a request ID so you can trace it all the way back and forth right give that as a recipe going forward same with Q system same with serverless so on and so forth so then teams are empowered to say okay if I want unic cost my app fits into this bucket these are the four things I should do and then I can go and look at a single pan of glass and it'll start popping up there and the last thing I'm looking at is finops governance is code and this one is really interesting because what I find is um we have co uh governance already which prohibits people from say deploying a new service from reinvent immediately right there's a lot of checks we have to do security we have to harden the module and terraform but more and more often what happens is the engineers the actual ec2 as an example you know we can Whit list to say yes you can use ec2 going forward but the engineer is still responsible for the type right and now imagine engineer you know doesn't know any better and says oh yeah the P5 instance looks awesome I want to do some AI stuff well it's $98 an hour he goes home for the weekend comes back Monday and then you know we're in this like call with the MD saying what happened here so instead of that what I really want to be able to do is have governance as code which prevents those things from happening and it tells you why it's happening and if you still want to use it this is what you have to do you know get your product person or your financial sponsor agree to it and then we can remove that block there going forward I know tools like this exist already in the industry but what I'm really finding is for every vendor that's out there solving a lot of these problems there's no kitchen sync right and so some of these things may be solvable by an open source product some of them might just be things we have to do documentation on or some of these things might be uh you know things that we just don't have an answer for today uh with that I'll open up for questions yes thank you so much great thought absolutely thank you any questions there there are no Runners J say I'll do it anyway you're right there there you go so I'm just I'm real Curious around the close around F's governance as code like can you and give some examples around that yeah it could be things that uh as a as a team that's Central you start partnering with not just you know finops but you think about a platform and you want to enable some services and enable specific flavors of those Services right so we have that today in security right we don't allow for example Opa policies to have S3 buckets which are unencrypted we don't allow you know albs to be Ingress only which have open to the internet we don't really have those kind of controls yet around fops right we can have buckets which don't have a life cycle policy on them right which is again not a security concern but it's a concern for things that matter to cost right so thinking about those things and instead of always having a reactionary appro excuse me approach where you kind of look at you know some vendor which gives you all the data and then you kind of find it and then you go back you ask them hey can you fix it why don't we have advisory policies that are always updated we say hey this is going to go hard mandatory soon uh get off of this instance because we are now going to switch to am your Nitro right so this way at at development at part of thec pipeline you have that available instead of just afterward and then you go into the whole should we raise a juror ticket the team is not optimized to do this it's going to take a while we catch it before so the shift left actually starts occurring thank you other questions yeah hold I'll give it to you similar to the question that she had around the fups as of governance uh two things which teams do you work with on fups as a governance and what tool do you use yourself uh if I speak next year I'll let you know because we haven't started yet okay so it's something you're working towards yeah exactly because I feel like a lot of the things you were saying we're trying to do them but we didn't call it finops as governance right so now that you you know nice name to it so that people can understand but just want to know which teams we should focus to work I think architecture the I think the idea really is architecture right you you think about what should be prevented from being deployed and not from a security context well the owners of this application and The Architects of this application you go to them and you explain to them well did you know this cost is double if you use this do you really need this instance right you almost like bringing awareness of finops at the architectural level so when you're considering a new platform You're Building you can do some analysis and figure out okay we could do Lambda we could do eks or fargate what is the Ben that we're really looking for outside of performance and cost becomes one of those things and then you make it the default and you let overrides happen right there'll always be teens out exception but you prevent this thing of you know often times Finance is like I have no idea what cloud cost even mean what is an ec2 instance and developers like we have to do this it's like no you don't you really don't need to be using this instance going forward there yeah you hear me all right oh yeah there we go so two parts um interested all the work you guys are doing in an effort to kind of break down those costs yeah what kind of resource capacity do you need to comish that on a monthly basis yeah so we actually do it on a daily basis um and when you say resources do you mean uh the my team structure or yeah like how many resources does it take for you to be able to drive that level of granularity for your people what so when I think about resourcing um I often find engineering is a precursor to under really understanding finops like pulling up the hood and not just doing click Ops right like a tool tells you to click a button you're like I guess I got to click it but if you understand how ec2s actually work how resourcing Works how terraforms happen or terraform actually deploys code it's much easier to find individuals who can learn fops then the opposite way around where you learn engineering overnight right so I think you know I can't say the specific amount but it's under 10 right and a lot of everyone I hire is really focused on the engineering side or the product side and less on the sourcing or the financial side yeah hey I have one quick question about unit cost so how do you define the unit cost for uh two projects are sharing the same back end like Kafka or Cassandra something so how do you define the unit cost for each project if you want to do it that's a really good question and that's something that you know as we're building a platform I can't answer right depends on the team depends on if it's a multi tendent cluster or not depends on whether they consider this cost to be just the cost of doing business or they want to use this to now Drive Revenue numbers and want to understand what they want to price out per million API calls or something but what we can do is kind of give t uh teams a tooling to say here's an example project and the way we got this data is because on every ex request header that comes in we had a guid and we Trace that all the way down to comes back up the stack and then what we can do is and you know maybe using Splunk or elk stack look at those number of 200 API calls at a response time and then work out from there right so it's there's really no Silver Bullet for unic cost kind of complex use case yeah exactly we we've done it for you know some of our bigger applications but it's not scalable right so when a lot of it sometimes I feel like I want to automate myself right on the proverbial half hour meeting where I'm explaining these things and you know documentation is not enough it's like well we kind of need code that kind of shows these recipes so what we look at are the biggest applications that are running and what we did we turned them into case studies internally yeah hi I wanted to T more about the savings plan question oh yeah so I'm curious our savings plan currently managed uh and purchased at the team level or essentially by a finance team or engineering team and why is it that way yeah so we do today uh at the linked account level by the team and the reason for that is uh we need a commitment internally that this team will continue using either the CSP itself or the uh the region or specifically the instance if you go down to that level and in order to do that this team also wants to benefit first right so if you purchase at the payer level centrally what we don't get is any sort of commitment from somebody and we can't guarantee this individual team account or accounts are going to get that benefit first so of course we lose imagine a little bit of value because this SP engine would find the biggest savings every hour but we gain is now data actually makes sense and the team that purchased SP is actually getting it there yeah cool any more questions no well massive Round of Applause for a meet thank you so much thanks for watching check out more finops X 2024 content on our YouTube channel on the 2024 playlist support our Channel by liking subscribing clicking the notification Bell and by leaving comments and questions for our speakers we appreciate your support
```

## Transcript 6
```
sharing the architecture and content of a finops data Lake to enable Advanced bi please welcome Laurent kiss thank you so much thank you thank you so much it's great to have you here actually as Joe mentioned my name is let's let's go another one yeah let's go another one yeah exactly that's what I want all right we will be running out of time so [Applause] okay so my name is Laura and Kish and I'm the Phoenix engineering manager of Amadeus and actually today I brought you a presentation about our data Lake an analytical solution that will we've built I'm going to talk about the architecture of it the Conan of the lake as well as I'm going to show you a number of screenshots from our reporting before that however I want to introduce a bit our company because it's business to business you might not know who Amadeus is so who of you have actually flown in to attend the event by a plane okay quite many people so rest assured that behind the services Amadeus has been helping you for buying your ticket for checking in your luggage or boarding the aircraft we are actually in the travel domain 500 airlines are cost close to 500 Airlines our customers but also ground handlers also more than one million hotel properties are managed through Amadeus systems to make this happen we are actually um historically using a data center in Germany which process is 1 million business transactions per second actually my previous role I was manager for SRE managing 120 000 transactions per second it's pretty awesome component and in this data center we have tens of thousands of physical and virtual machines and we set off on a cloud journey in 2017 to move to the public cloud and as the cloud enablement started actually in 2018 we built ACS Amadeus cloud services which is our internal kubernetes based containerization platform in 2020 actually we launched our phenops operation in 21 we went into a strategic partnership with Microsoft however we are still a multi-cloud provider business we have quite a footprint still in AWS and gcp so it's not exclusive and in 2022 actually we released just about a year ago the system that I'm going to show you now we are entering the adoption phase um as well as the mass migration I will start actually with a key takeaway from our previous talk you actually can have a look at it on YouTube it's from the April Summit and in this presentation we explained a little bit the thinking about our finops and one thing I would like to point out here that often people ask the question whether to build or to buy something in my view this is probably the wrong question it's more analyzing what you need what are the components you want to do and more ask the question what to build and what to buy so in our case because we had some special requirements about coastal location or distributed forecasting this component the data component and the reporting component we have actually built ourselves so let's move on to the architecture of this and this is a little bit more than the lake it's the complete analytical solution and treat this as a snapshot it's every architecture is evolving um the data here travels actually from the left to the right you see that the data source is obviously the bill is one of the biggest ones you will see we have lots of other data entering the lake so this goes into another data Lake storage generation too mostly the data is stored in CSV as well as Market format and we are processing this with serverless technology as we're functions also the orchestral orchestration layer on top is azure functions durable functions as they call it and then the right two boxes are really for serving up the data the synapse sqs serverless is actually not storing anything it's really there to provide you an SQL interface to reading your data in the lake and eventually the data travels into an in-memory database or data set in power bi which enables you really fast reporting and power bi is also the platform that we use to build up the reports dashboards and put it into a navigatable application so let's move on zoom on a little bit on this Lake what are some key facts and figures so we have about 100 containers 100 different schemas in this data Lake schema actually means one sort of a table or one sort of a hierarchical structure today we have 250 000 files or binary long objects blobs in this data Lake which occupy seven terabytes of storage again they are before the mass migration so you can expect that this this data will actually grow quite a lot we are targeting a 37 months active retention those of you who deal with machine learning trying to forecast inferred patterns from time series know that one year of retention will not do it because you need to have multiple years to understand the patterns over the year the next two numbers compression ratios one to ten and one to five this is how much you can save if you actually store your data for example in Pocket format instead of CSV this is a 90 saving our experience and if you compress your data from a daily to a monthly this is giving you a one to five in our experience one would expect that probably is one to Thirty it's not because you have a lot of Dimensions you aggregate with it and actually uh with this you you go into much more rows so one two five is in our experience what's working last but not least the seven is actually our number of our SLO metrics we really act like an SRE or devops team and redefined availability as well as six other metrics mostly for data quality like completeness or freshness of the data to track for ourselves so how does the data get into the lake the data enters three ways either very simply you go to the cloud provider portal you enter some configuration you put your Target location and the data will appear automatically this is typical for the bills then we have data that we actually pull for ourselves we operate the code to reach out either internally or externally to the to the internet to documents to apis and take snapshots typically we take daily snapshots so that we have a historical understanding of this and last but not least we have cases when we really just provide the container we set up the right permissions for another team and it's up to them to push the data to us this is our case for example for coastal location keys I wanted to also tell you about six thoughts or things it's interesting to think about when you build a data Lake we talked already about the container so the separation on a schema layer is interesting because you want to be both secure and you don't want to go into a mess so I think separating it not just on so to say folder level but container level is a good practice second Point partitioning typically you don't want huge data in one monolith so you partition it our partitioning scheme is time based so we have years months as folders and typically the files are separated day by day but you can use other dimensions for partitioning it will happen to you that the provider of the data will change the name of the column your pipeline breaks so you need to prepare for versioning it in fact in one container we have multiple versions of the same data as it evolves over time and then you can control it also from your code storage for storage you know the shape of the finops data or or the usage of the finance data is like you you put it in the lake you write it once you don't really modify it but then you query it million times and you keep it quite long so for this shape of data based on today's best wisdom you use a columnar store which is compressed such as the park at format but there are also alternatives semantics for semantics it's it's to say that when you have a cryptic name of a column and you want to expose it to your business users not sure they will understand in controlling or in product management what the engineers meant by a cryptic name so it's always interesting to think about the end user in mind and also later on natural language processing to have meaningful names for your columns from the start and last but not least a data mesh strategy Amadeus has a data Mass strategy and I consider phynops as a as a target for finops to be one of the nodes in this data mesh so that the business teams can actually use the data to mix it with their own all right let's let's go a little bit into what's in the lake because I think it would bore you to hell to have 100 containers you know one by one going through the slides I did a bit of grouping of them looking at the Grid on the top with these four items I used actually two Dimensions one of them whether the data is of internal source so from Amadeus or from external source the other is whether the data is basic or really more beyond the obvious that's what I called Advanced here and last but not least the fifth part on the bottom is really operational data which is so to say generated in and from the lake itself so let's start by the external and obvious you will want to put your cloud provider builds into the data Lake if you're multi-provider you want to put them as well for those of you who are operating for example multiple enrollments in Azure you probably want to separate them into different containers as well as you want to do amortize and actual data download separately we do post-process these bills we have derived augmented products what we call the curated bill so after cleaning um and and enrichment we actually put that into a new container I'm really looking forward for Focus to solve much of our problems here actually and we also do cost allocation mechanism and furthermore an expanded cost to be able to drill on drill down into the old location now what's what's beyond the obvious and these sources are all from the provider or from the open internet orphaned resources this provides you a savings opportunity that is not obviously appearing in the cloud provider recommendations or in the bill itself I just saw a presentation from Adobe it was great to see that they managed to even automate this and achieve tremendous savings for us actually we are reaching out to four different apis and identified for example the allocated VMS on attached disks orphan resources and we pull them into the data Lake our obvious thing is probably the crowd provider recommendations But be sure to make your own calculations because what we found is that often they are not considering some of your discounts and it's interesting to see the actual potential and not be mistaken utilization your engineers will be super interested in the utilization for example for VMS you could imagine both the CPU and the memory utilization and its Evolution over time to understand what are the shapes of the workloads and do optimization beyond the recommendations sustainability Cloud providers now provide CO2 data typically it's not granular enough we actually use our coastal location process to attribute it to the different services commitment-based discounts both the transactions of reserved instances savings plans commitment committed use discounts as well as the recommendations is interesting to pull together so that you can model financially better CSP metadata I mean I pulled Azure examples here as I mentioned we are a Microsoft partner and for this reason we are also heavily investing into Azure so in their case for example for the flexibility of reservations they provide this very cryptic name tables instant size flexibility group and autofit combo meter if you're in Azure you might want to look into this and last but not least pricing your price sheet and public price list will help you to understand so to say the street price versus what you're getting at the end and maybe even drill down into the details of your of your discounts whether it's from the reservations whether it's from foreign exchange rate gains or whatsoever let's focus internally now in Amadeus actually the language that our people speak revolves around the first two boxes here products and services product is for example the altea suit which we offer to Airlines and this is a term that's very familiar to to marketing people to product managers underneath you have multiple services like in our case customer management to board your aircraft or flight management to departure aircraft and check your luggage uh which are more technical applications this is what we call services here and underneath you even have teams which serve these applications with a database or with containerization environments so all of this is cataloged in Amadeus in separate systems and this is something we're also snapshotting on a daily basis and this forms key dimension for the reporting so that everybody understands the language foreign exchange rates as we are building multiple currencies sometimes US Dollars sometimes Euros you want to make sure that you're controlling team Finance team is aligned in the way you're converting those currencies and you can report both ways zoning zoning is actually a concept that we internally developed to encourage or discourage the use of certain types of resources we do this for example for VM families as well as different regions because of it could be the price it could be the life cycle of the product we might prefer or not prefer certain types we use here green Amber red black categories and this is also evolving over time and the last two just to mention um it's more a bit of you know metadata or configuration data to manage your systems can be groupings of things can be whitelists for example if you are doing tag processing and your organization uses thousands of text but infinix you're interested only in 100 probably you want to whitelist them and work only with that part of the data okay the more advanced or specific to us is that we are asking our service teams and application teams to provide us data about how they are used for example if there is a database team which is used by many business applications we ask the database team to provide us custom location keys meter the usage of their customers internally just like you would be a provider and from products we also ask for for North Star metrics for example in our case passengers boarded or flights departed can be a very good North Star metric one highlight case here is the is the pass so our containerization platform on kubernetes openshift here we really rely quite a lot on Prometheus monitoring data or observability data for three use cases to understand how much people requested and compared to that how much they used to make sure that the coastal location is also fair and based on this data and also to correct the labels that they are using in the past and last but not least the operational part the top one the direct one is really data that the report consumers use or the data set consumers use these can be for performance reasons pre-aggregation pre-calculation or for example detection of anomalies and the logs of the notifications the supporting data is more for our phenops engineering team so that's to understand what are the container sizes in the lake how does the data look like for example Great Expectations data validation results test results because we are also looking end to end to the system so we are testing the UI or anomaly detection metadata or other configurations all right so we get to the point of the showcase a few reports this one is actually a cost breakdown for an internal service so you could imagine you're in an engineering team which operates a database or operates our customer management you can come to this piece of the report you can see actually on the top you have different personas so we are in the engineering Persona and we are looking at cost by provider but what we mean by provider here is not just the cloud provider but as mentioned we do the all locations so if your customer management application and you're using a database underneath or containerization you will see the breakdown of this data it's kind of your internal bill on the bottom you see actually different breakdowns the four we are mostly using are instances of the service environment of the service Architectural Components as well as partitions if the application happens to be partitioned for high availability or compliance reasons one interesting feature actually that comes with the power bi platform is the ability to explain an increase or decrease for this to work obviously your data has to be in in a specific shape but what you can do as you you see this timeline on the line chart there was a big increase right clicking on the top data point you will get suggestions or ideas the system identifies as most likely to contribute to this increase and in this case I could see in this list which environment in production and which component happens to be a redis component that was causing this increase so it saves you some time by not having to drill for yourself and try to search for the reasons I'm not saying it always work but it oftentimes gives some good idea unit cost related capabilities so here we are mixing the organizational metadata the service and instance data with the coastal location and the bill itself so you see actually on on the top left the evolution of the unit cost below that you see the total cost and the total volume so just you know the numbers you calculate the unit cost from and the interesting bit is actually the bubble chart the bubble chart shows the instances for example if there's a database team it shows the instances of the database different clusters that are served to other applications and this chart can be used to identify first of all the economies of scale because you would expect the higher the volume the lower the unit cost so it should show up on the chart accordingly and it helps you also to identify outliers like the big red bubble which is an instance which is both high in volume and high in unit question you might want to look into that and compare it to the blue ones below because they are pretty low unit cost and probably they are doing something better than the others the mentioned cost avoidance or fan resources so here pulling the data from different apis we can for a service team identify the savings opportunities of these four categories that you you see on the screen so the VMS the storage snapshots and IPS and we also have on the bottom table the capability to drill down and then a direct link actually to the cloud provider portal so if you want to take action you can directly go there VM zoning this is the mentioned zoning concept here I actually filtered this report for three different database technology teams that's what you see on the top bar chart and you can see that the biggest spender team is quite doing well because they have a lot of footprint in the recommended green zone whereas the second one is really falling behind with the Amber and red so something needs to happen there of course there are drill down capabilities into this and last but last but not least a little bit more uh Advanced or forward-looking capabilities actually we use cognitive services to detect anomalies how we do this is that we buy Service Supply the daily total cost allocated cost to the system for each service there is a model built I mean the model very simply you can see the thicker Blue Line in the Middle with the top and bottom margins on the side and if you're within this range within this tunnel you're good if you're going outside like the big spike there we are able to send a notification to the team something needs to be looked at the bar chart one the horizontal bar chart one is more experimental but it's kind of forward-looking to ask natural language question and the system generates for you the proper visuals or gives you the proper number this is not something ready for production but we are experimenting with it and this I mean I would refer back here to the semantics because it's interesting how people will ask this question and how the system will understand it but it's it's a good exercise and I think for the future it's interesting to prepare with that all right I'm ready to wrap up so key takeaways here there is really a lot more to finish data than just the bill so I think it's interesting to pull together your data assets I think finops can be a game changer and even go beyond phynops go to financials second point I personally believe is that modern day in analytics and bi tooling makes it easier than ever before don't be mistaken I'm not saying this is a child's play but it's possible and if you're pulling together a few Engineers with today's technology you can handle quite large data sets meaningfully and one more thing it's more referring back to the April Summit and our coastal location code which we actually Outsource open sourced so feels feel free to look at the video and also just simply type Amadeus finops in the browser search bar and you will have on the first page results both the video and the coastal location keys and I think that wraps it up thank you we have a little time are you okay with questions yeah we can go for questions anyway all right I like it's right here hello great presentation thank you um I had two questions one what what's your retention period for the data that you do keep on store in the data Lake for all the all Cloud costs and then two funny story um data lake has actually been one of our top like cost problems within the cloud so did you get any pushback from like executive leadership to use data Lake as the tool to then reduce costs in the cloud right okay okay and if you did what persuasion metrics this question I mean I mentioned this 37 months for active retention that's to you know work with the data but if you're asking you know how long otherwise I would say compliance legal compliance tax compliance should give the answer I think it's in the range of five to seven years typically what's you know required but people often keep it even longer we don't have that long data even even we don't reach the 37 months yet yeah so irrelev over time but that's that's what we have in mind for the other question can you remind me push back okay okay cost of the system and push back no actually my boss sent me a concrete limit Lawrence you cannot spend more than this per month on this one we set it in the tool and we keep it no actually you know you have to make your own calculation but according to our calculation it was the price yeah a techie question actually two because she has to uh what do you use for the query engine and how big is your team excuse me what do you use for the query engine I'm assuming you're using trino or something like that too okay so yeah I mean I would differentiate two use cases um because we offer the reports for end users and we offer the data set as well for more advanced people who want to build their own reporting so data as a service kind of or data as a product kind of thing and actually you can enter this on I would say three layers in this architecture you can either directly you know use the lake data which is for investigation for you know very weird things like special character blocks the thing you can enter on the synapse layer so if you're a data as a product user you would enter to synapse synapse is not storing anything it provides an SQL interface you create views just like you would on your traditional database but it's on pocket files for example and that's that's what you can use then when we talk about the in-memory part so you have the power bi data set and for that you have Microsoft specific decks query language more analytical language there you can build measures and other stuff so this this is the three ways we could offer I prefer we go from uh you know the right to the left depending on the use case so first people really use the power bi data set and if it's not sufficient then we go back back and how many people on your team it's kind of a one and a half Pizza team well the finops team all together I mean in European terms it's probably sizable in your American terms it's not like this you know let's say about 15 people I don't know exact count out of this the engineering team is more like six core Engineers so yeah roughly that's that's how I would put it um I'm curious what kind of tagging strategy have you implemented to help support this kind of reporting can you speak can you talk about the tagging strategy that you've implemented to help support this kind of uh report can you repeat me the questions tagging strategy tagging strategy okay so yeah that's something we yeah much earlier started with and not Phoenix alone I mean there was also a time when people call this finops text it's not Finn up Stags it's Cloud tags security equally interested and several other groups uh its at management whatsoever so we actually tried to pull together these people and form the board so we in that sense we don't have this Cloud Center of Excellence but more an autonomous board from all the people who are interested who actually put out a policy and this policy is then checked you know against what we see in the cloud and we track a kpi on the top level how people are doing this what we haven't implemented yet but I would find interesting is to have a combination of tags and say whether you know this resource is finops compliantly tagged because security is interested in another set of tags than us it has an overlap but it's not completely the same and we could evaluate a resource as well tagged or not well tagged not just looking at a single tag and then you know presented to to the users that way that would be an interesting evolution that's awesome that's about as much time as we have for questions you can catch up with Laurent on the aircraft carrier tonight you could also go on finnops.org to search for his previous talk that he referenced uh also on the YouTube page where you can find last year's finopsex sessions and where we will upload these recorded sessions this year so let's out do breakout room a uh Stacy case over there so we got to drown her out everyone give a big round of applause for Laurence thank you thanks for watching that session I'm sitting here in San Diego right after fenopsex we hope you join us next year here live 2024. in the meantime please subscribe to the channel and join the community get involved join the Summits get in a working group and don't forget to get fin op certified because next year here in San Diego for fin op sex it's going to be twice as big come join the party come meet your people welcome home
```



1. Ingestion & Normalization
	1. Hosting Fees
	2. Labor
	3. (Optional) Reprocessing & Business Logic Correlation
		1. Tagging data
		2. Ingestion of other systems CMBD, AD, Observability tooling, etc. 
			1. More expertise
			2. More hosting costs
			3. FOCUS doesn't solve all this, so for larger/more complex, you might need a data fabric tool (SnowFlake, etc) or custom converters. 
2. Reporting & Visualization (baseline cost reporting and some allocation)
	1. BI tooling licensing 
	2. Creation & Maintenance (people cost)
3. Budgeting & Forecasting
	1. Budgeting
		1. (Required at this level) Reprocessing & Business Logic Correlation
			1. Tagging
			2. Ingestion of other systems CMBD, AD, Observability tooling, financial systems etc. 
				1. More expertise
				2. More hosting costs
				3. FOCUS doesn't solve all this, so for larger/more complex, you might need a data fabric tool (SnowFlake, etc) or custom converters. 
	2. Forecasting
		1. Basic
			1. In-BI optionality 
			2. Or, custom made formulas / logic
			3. More people, more risk
		2. Moderate
			1. Machine Learning
				1. Cloud hosted model with new datastreams
				2. Snowflake or Databricks model 
			2. More people, more risk
4. Optimization & Insights 
	1. Basic
		1. Port over via API the cloud native insights
	2. Moderate 
		1. Port over via api with custom joins to correlate to business
		2. Alerting
			1. Pager Duty, Slack, or other integration
	3. Advanced 
		1. Actually processing observability data
		2. Building custom algorithms and rules
		3. All of this requires more storage, more resources, more cache, more everything 
5. Remediation Actions
	1. Low-Code/No-Code
		1. GitHub Actions, Lambda, etc
		2. IaC
6. *and all other*